<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Stan Zhou&#39;s Hexo Technical Blog</title>
  
  <subtitle>Innovation Evangelist</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://223.95.78.227/"/>
  <updated>2020-04-26T12:51:17.995Z</updated>
  <id>http://223.95.78.227/</id>
  
  <author>
    <name>Stan Zhou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>puppet</title>
    <link href="http://223.95.78.227/2020/04/26/puppet/"/>
    <id>http://223.95.78.227/2020/04/26/puppet/</id>
    <published>2020-04-26T12:41:23.000Z</published>
    <updated>2020-04-26T12:51:17.995Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Puppet-Code-Directory-Layout"><a href="#Puppet-Code-Directory-Layout" class="headerlink" title="Puppet Code Directory Layout"></a>Puppet Code Directory Layout</h1><p>environments/production/:<br>manifests/:Puppet manifests<br>modules/: Puppet modules<br>data/: Hiera data<br>hiera.yaml: Hiera configuration</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Puppet-Code-Directory-Layout&quot;&gt;&lt;a href=&quot;#Puppet-Code-Directory-Layout&quot; class=&quot;headerlink&quot; title=&quot;Puppet Code Directory Layout&quot;&gt;&lt;/a&gt;Pu
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>i3</title>
    <link href="http://223.95.78.227/2020/03/16/i3/"/>
    <id>http://223.95.78.227/2020/03/16/i3/</id>
    <published>2020-03-16T03:53:08.000Z</published>
    <updated>2020-03-16T03:58:13.025Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>Key</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>$mod+shift+direction</td><td style="text-align:center">swap two windows</td></tr><tr><td></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Key&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$mod+shift+direction&lt;/td&gt;
&lt;
      
    
    </summary>
    
    
      <category term="i3, linux" scheme="http://223.95.78.227/tags/i3-linux/"/>
    
  </entry>
  
  <entry>
    <title>spacemacs</title>
    <link href="http://223.95.78.227/2020/03/03/spacemacs/"/>
    <id>http://223.95.78.227/2020/03/03/spacemacs/</id>
    <published>2020-03-02T23:08:10.664Z</published>
    <updated>2020-05-05T12:54:59.027Z</updated>
    
    <content type="html"><![CDATA[<table><thead><tr><th>Key</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>SPC p f</td><td style="text-align:center">search find in the same project</td></tr><tr><td>SPC p p</td><td style="text-align:center">open project</td></tr><tr><td>SPC w m</td><td style="text-align:center">max the current windows</td></tr><tr><td>ALT ;</td><td style="text-align:center">uncomment</td></tr><tr><td>gcc</td><td style="text-align:center">comment out a line</td></tr><tr><td>gc</td><td style="text-align:center">comment out highlighted text</td></tr><tr><td>SPC w /</td><td style="text-align:center">vertical</td></tr><tr><td>SPC w -</td><td style="text-align:center">horizontal</td></tr><tr><td>SPC f e d</td><td style="text-align:center">edit spacemacs profile</td></tr><tr><td>SPC f f</td><td style="text-align:center">open file</td></tr><tr><td>SPC w c</td><td style="text-align:center">close the window</td></tr><tr><td>SPC t l</td><td style="text-align:center">toggle lines</td></tr><tr><td>C-x h</td><td style="text-align:center">select the entire buffer</td></tr><tr><td>C-h m</td><td style="text-align:center">show available commands for the current modes</td></tr><tr><td></td></tr></tbody></table><h1 id="Stage-files-and-commit"><a href="#Stage-files-and-commit" class="headerlink" title="Stage files and commit"></a>Stage files and commit</h1><table><thead><tr><th>Key</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>SPC g s</td><td style="text-align:center">show Magit status view</td></tr><tr><td>j k</td><td style="text-align:center">position the cursor on a file</td></tr><tr><td>TAB</td><td style="text-align:center">show and hide the diff for the file</td></tr><tr><td>s</td><td style="text-align:center">stage a file (u to unstage fa file and x to discard changes to a file)</td></tr><tr><td>cc</td><td style="text-align:center">commit</td></tr><tr><td>,c</td><td style="text-align:center">finish the commit message</td></tr><tr><td>p u</td><td style="text-align:center">Push to upstream</td></tr></tbody></table><h1 id="Org-mode"><a href="#Org-mode" class="headerlink" title="Org mode"></a>Org mode</h1><table><thead><tr><th>Key</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>M-x org-mode</td><td style="text-align:center">enable Org mode</td></tr><tr><td>M-shift-RET</td><td style="text-align:center">Add a TODO list</td></tr><tr><td>C-c C-t</td><td style="text-align:center">Mark as completed</td></tr></tbody></table><h1 id="tumx"><a href="#tumx" class="headerlink" title="tumx"></a>tumx</h1><table><thead><tr><th>Key</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>prefix :setw synchronize-panes off</td><td style="text-align:center">stop sending same command for all paines in windows</td></tr><tr><td>prefix r</td><td style="text-align:center">reload the tmux configuration</td></tr><tr><td></td></tr></tbody></table><p><a href="https://orgmode.org/manual/index.html">https://orgmode.org/manual/index.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Key&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SPC p f&lt;/td&gt;
&lt;td style=&quot;tex
      
    
    </summary>
    
    
      <category term="spacemacs" scheme="http://223.95.78.227/tags/spacemacs/"/>
    
  </entry>
  
  <entry>
    <title>monitoring</title>
    <link href="http://223.95.78.227/2020/02/26/monitoring/"/>
    <id>http://223.95.78.227/2020/02/26/monitoring/</id>
    <published>2020-02-26T10:52:07.000Z</published>
    <updated>2020-03-19T06:36:08.726Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Frontend-Monitoring"><a href="#Frontend-Monitoring" class="headerlink" title="Frontend Monitoring"></a>Frontend Monitoring</h1><h2 id="Two-approaches-to-frontend-monitoring"><a href="#Two-approaches-to-frontend-monitoring" class="headerlink" title="Two approaches to frontend monitoring"></a>Two approaches to frontend monitoring</h2><ul><li>real user monitoring (RUM)</li><li>synthetic<ul><li>tools: webpagetest.org<br><strong>Browsers expose page performance metrics visa the Navigation Timing API</strong></li></ul></li></ul><h1 id="Application-Monitoring"><a href="#Application-Monitoring" class="headerlink" title="Application Monitoring"></a>Application Monitoring</h1><p><strong>Application Performance Monitoring(APM) Tools</strong></p><ul><li>StatsD is a tool used to add metrics inside of your code.</li></ul><h1 id="Telegraf-InfluxDB-Grafana"><a href="#Telegraf-InfluxDB-Grafana" class="headerlink" title="Telegraf, InfluxDB + Grafana"></a>Telegraf, InfluxDB + Grafana</h1><ul><li><p>Grafana: Grafana is “The open platform for beautiful analytics and monitoring.” It makes it easy to create dashboards for displaying data from many sources, particularly time-series data. It works with several different data sources such as Graphite, Elasticsearch, InfluxDB, and OpenTSDB. We’re going to use this as our main front end for visualizing our network statistics.</p></li><li><p>InfluxDB: InfluxDB is “…a data store for any use case involving large amounts of timestamped data.” This is where we’re going to store our network statistics. It is designed for exactly this use-case, where metrics are collected over time.</p></li><li><p>Telegraf: Telegraf is “…a plugin-driven server agent for collecting and reporting metrics.” This can collect data from a wide variety of sources, e.g. system statistics, API calls, DB queries, and SNMP. It can then send those metrics to a variety of datastores, e.g. Graphite, OpenTSDB, Datadog, Librato. Telegraf is maintained by InfluxData, the people behind InfluxDB. So it has very good support for writing data to InfluxDB.</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Frontend-Monitoring&quot;&gt;&lt;a href=&quot;#Frontend-Monitoring&quot; class=&quot;headerlink&quot; title=&quot;Frontend Monitoring&quot;&gt;&lt;/a&gt;Frontend Monitoring&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>SRE</title>
    <link href="http://223.95.78.227/2020/02/09/SRE/"/>
    <id>http://223.95.78.227/2020/02/09/SRE/</id>
    <published>2020-02-09T01:10:52.000Z</published>
    <updated>2020-02-09T06:18:34.749Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Implementing-SLOs"><a href="#Implementing-SLOs" class="headerlink" title="Implementing SLOs"></a>Implementing SLOs</h1><p>Service level objectives (SLOs) specify a target level for the reliability of your service.<br>SLOs are key to making data-driven decisions about reliability, they’re at the core of SRE practices.</p><p>SLOs are a tool to help determin what engineering work to prioritize.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Implementing-SLOs&quot;&gt;&lt;a href=&quot;#Implementing-SLOs&quot; class=&quot;headerlink&quot; title=&quot;Implementing SLOs&quot;&gt;&lt;/a&gt;Implementing SLOs&lt;/h1&gt;&lt;p&gt;Service le
      
    
    </summary>
    
    
      <category term="SRE" scheme="http://223.95.78.227/tags/SRE/"/>
    
  </entry>
  
  <entry>
    <title>photography</title>
    <link href="http://223.95.78.227/2019/09/28/photography/"/>
    <id>http://223.95.78.227/2019/09/28/photography/</id>
    <published>2019-09-27T14:44:32.492Z</published>
    <updated>2019-09-27T14:44:32.492Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Exposure</strong>: Aperture, ISO speed, and shutter speed are the three core controls that manipulate exposure. </p><p><strong>Camera metering</strong>: The engine that assesses light and exposure. </p><p><strong>Depth of field</strong>: An important characteristic that influences our perception of space.</p><h2 id="Understanding-exposure"><a href="#Understanding-exposure" class="headerlink" title="Understanding exposure"></a>Understanding exposure</h2><h3 id="Exposure-triangle"><a href="#Exposure-triangle" class="headerlink" title="Exposure triangle"></a>Exposure triangle</h3><p><img src="https://i.imgur.com/qViBe1j.jpg" alt="the exposure triangle"><br><strong>Shutter speed</strong>: Controls the duration of the exposure</p><p>The shutter spped, or exposure time, refers to how long this light is permitted to enter the camera.</p><p>Range of Shutter Speeds<br>SHUTTER SPEED|TYPICAL EXAMPLE<br>—|—<br>1 to 30+ seconds|To take specialty night and low-light photos on a tripod<br>1/2 to 2 seconds|To add a silky look to flowing water landscape photos on a tripod for enhanced depth of field<br>1/30 to 1/2 second|To add motion blur to the background of moving-subject, carefully taken, handheld photos with stabilization<br>1/250 to 1/50 second|To take typical handheld photos without substantial zoom<br>1/500 to 1/250 second|To freeze everyday sports/action, in moving-subject, handheld photos with substantial zoom (telephoto lens)<br>1/8000 to 1/1000 second|To freeze extremely fast, up-close subject motion</p><p><em>Note that the range in shutter speeds spans a 100,000× ratio between the shortest exposure and longest exposure, enabling cameras with this capability to record a wide variety of subject motion.</em></p><p><strong>Aperture</strong>: Controls the area through which light can enter your camera<br>A camera’s aperture setting controls the width of the opening that lets light into your camera lens.<br>We measure a camera’s aperture using an f-stop value, which can be counterintuitive because the area of the opening increases as the f-stop decreases. For example, when photographers say they’re “stopping down” or “opening up” their lens, they’re referring to increasing and decreasing the f-stop value, respectively<br><img src="https://i.imgur.com/BSiRQZX.jpg" alt="F-stop values and the corresponding aperture area"></p><p><strong>ISO speed</strong>: Controls the sensitivity of your camera’s sensor to a given amount of light</p><p><img src="https://i.imgur.com/5R1Mq0c.png" alt="Typical Focal Lengths and Their Uses"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Exposure&lt;/strong&gt;: Aperture, ISO speed, and shutter speed are the three core controls that manipulate exposure. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>pipeline</title>
    <link href="http://223.95.78.227/2019/09/26/pipeline/"/>
    <id>http://223.95.78.227/2019/09/26/pipeline/</id>
    <published>2019-09-26T04:48:03.587Z</published>
    <updated>2019-09-26T04:48:03.587Z</updated>
    
    <content type="html"><![CDATA[<p>Pipeline</p><p>Represents a part of:</p><ul><li>Software delivery</li><li>Quality assurance process</li></ul><p>Benefits:</p><ul><li>Operation grouping</li><li>Visibility</li><li>Feedback<br><strong>Automated deployment pipeline</strong><br>3 stages:<br>code change-&gt; Continuous Integration-&gt; Automated acceptance testing-&gt; Configuration management</li></ul><ol><li>Continuous Integration<br>The continuous integration phase provides the first feedback to the developers. It checks out the code from the repository, compiles it, runs unit tests and verifies the code quality. If any step fails the pipeline execution is stopped and the first thing the developer should do is fix the continuous integration build. The continuous integration pipeline is usually the starting point.</li></ol><ul><li>first feedback</li><li>checks code</li><li>starting point</li><li>simple to setup</li></ul><ol start="2"><li>Automated acceptance testing</li></ol><ul><li>Suites of tests</li><li>A quality gate</li><li>Pipeline execution is stopped if test fails</li><li>Prevents movement</li><li>Lot of confusion</li></ul><p><img src="https://i.imgur.com/KGWFQPx.png" alt="Agile testing matrix"></p><ol><li>Acceptance testing</li></ol><ul><li>Represent functional requirements</li><li>Wrtten in the form of stories or examples</li></ul><ol start="2"><li>Unit Testing</li></ol><ul><li>Provide the high-quality software</li><li>Minimize the number of bugs</li></ul><ol start="3"><li>Exploratory Testing</li></ol><ul><li>Manual black-box testing</li><li>Breaks or improves the system</li></ul><ol start="4"><li>Non-functional testing</li></ol><ul><li>Represent properties:<ul><li>Performance</li><li>Scalability</li><li>Security</li></ul></li></ul><ol start="3"><li>Configuration management</li></ol><ul><li>Replaces the manual operations</li><li>Responsible for tracking and controlling changes</li><li>Solution to the problems</li><li>Enable storing configuration files<br>Configuration management is a solution to the problems posed by manually deploying and configuring applications on the production. Configuration management tools enables storing configuration files in the version control system and tracking every change that was made on the production servers.</li></ul><p><strong>Technical and development prerequisites</strong></p><ul><li>Automated build, test, package, and deploy operations</li><li>Quick pipeline execution</li><li>Quick failure recovery</li><li>Zero-downtime deployment</li><li>Trunk-based deployment</li></ul><p><strong>Building the continuous delivery process</strong><br>Jenkins</p><ul><li>Popular automation server</li><li>Helps create automated sequence of scripts</li><li>Plugin-oriented</li></ul><p>Ansible<br>Helps with:</p><ul><li>Software provisioning</li><li>Configuration management</li><li>Application deployment</li></ul><p>Java</p><ul><li>Most popular programming language</li><li>Develop with the Spring framework</li><li>Gradle-build tool<br>Jenkins is by far the most popular automation server on the market. It helps to create continuous integration and continuous delivery pipelines and in general any other automated sequence of scripts. Highly plug-in oriented it has a greater community which constantly extends it with new features. Another one is Ansible. Ansible is an automation tool that helps with software provisioning configuration management and application deployment. Next comes Java. Java has been the most popular programming language for years, that is why it is being used for most code examples. Together with Java, most companies develop with the Spring framework so we used it to create a simple web service needed to explain some concepts. Gradle is used as a build tool. It is less popular than Maven however trending much faster.</li></ul><p><strong>Pipeline elements</strong></p><ul><li>Basic elements</li><li>Stage - Logical separation</li><li>Setp - Used to visualize process<br><img src="https://i.imgur.com/J7xJ8KG.png" alt="elements"></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">   //agent any</span><br><span class="line">   agent &#123;</span><br><span class="line">      label &apos;linux&apos;</span><br><span class="line">   &#125;</span><br><span class="line">   stages &#123;</span><br><span class="line">      stage(&apos;First Stage&apos;) &#123;</span><br><span class="line">          steps &#123;</span><br><span class="line">              echo &apos;Step 1. Hello World&apos;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      stage(&apos;Second Stage&apos;) &#123;</span><br><span class="line">          steps &#123;</span><br><span class="line">              echo &apos;Step 2. Second time Hello&apos;</span><br><span class="line">              echo &apos;Step 3. Third time Hello&apos;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   post &#123;</span><br><span class="line">     always &#123;</span><br><span class="line">        echo &quot;Pipeline executed!&quot;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Define pipeline structure<br>A declarative pipeline is always specified inside the pipeline block and contains sections, directives and steps. </li></ul><p>A declarative pipeline has a simplified and opinionated syntax on top of the pipeline sub-system.</p><p><strong>Sections</strong></p><ul><li>Keywords are:<ul><li>Agent-defines on which agent the pipeline or the stage will be executed</li><li>Stages-defines the stages of the pipeline</li><li>Step</li><li>Post-defines the post build steps</li></ul></li></ul><p>Sections define the pipeline structure and usually contain one or more directives or steps. They are defined with the key words, stages, step and post.</p><p>Stages defines a series of one or more stage directives.</p><p>Steps defines a series of one or more step instructions.</p><p>Steps are the most fundamental part of the pipeline they define the operations that are executed so they actually tell Jenkins what to do.</p><p>Defined using:</p><ul><li>sh: executes the shell command</li><li>custom</li><li>script</li></ul><p>Posts defines a series of one or more step instructions that are run at the end of the pipeline build.</p><p><strong>Directives</strong></p><ul><li>Expresses the configuration of a pipeline or its parts</li><li>Defined by:<ul><li>Agent: specifies where the execution takes place</li><li>Triggers: define automated ways to trigger the pipeline and can use cron to set the time based scheduling</li><li>Options: specified pipeline specific options</li><li>Environment: deinfes a set of key values used a s environment variables</li><li>Parameters: define a list of user input parameters</li><li>Tools: defines the tools configured on Jenkins</li><li>Stage: allows for logical grouping of steps; contains steps and agent </li><li>When: determines whether the stage should be executed depending on the given condition</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">   agent any</span><br><span class="line">   tools &#123;</span><br><span class="line">      maven &apos;M3&apos;</span><br><span class="line">   &#125;</span><br><span class="line">  </span><br><span class="line">   parameters &#123;</span><br><span class="line">      string(name: &apos;VERSION&apos;,</span><br><span class="line">          defaultValue: &apos;1.0.0&apos;,</span><br><span class="line">          description: &apos;What is the version to build?&apos;)</span><br><span class="line">   &#125;</span><br><span class="line">  </span><br><span class="line">   stages &#123;</span><br><span class="line">      stage(&apos;Build&apos;)&#123;</span><br><span class="line">          steps &#123;</span><br><span class="line">             sh &quot;./build.sh $&#123;params.VERSION&#125;&quot;</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Commit-Pipeline"><a href="#Commit-Pipeline" class="headerlink" title="Commit Pipeline"></a>Commit Pipeline</h2><ul><li>Basic continuous integration process</li><li>Results in a report about the build</li><li>Runs after each change in the code</li><li>Consume a reasonable amount of resources</li><li>Starting point</li></ul><p>Since it runs after every change in the code, the build should take no more than five minutes and should consume a reasonable amount of resources. The commit phase is always the starting point of the continuous delivery process and it provides the most important feedback cycle in the development process.</p><p>In the commit phase a developer checks in the code to the repository.<br>The continuous integration server detects the change and the build starts.<br>The most fundamental commit pipeline contains three stages </p><ul><li>checkout</li><li>compile</li><li>unit test</li></ul><h2 id="Code-Quality-Stages"><a href="#Code-Quality-Stages" class="headerlink" title="Code Quality Stages"></a>Code Quality Stages</h2><p><strong>Extending continuous integration</strong></p><ul><li>The most widely used are: Code coverage and Static analysis</li></ul><p><strong>Code coverage</strong></p><ul><li>Scenario:<ul><li>Nobody writes unit tests</li><li>Passes all the builds</li></ul></li><li>Solution:<ul><li>Add coverage code tools</li></ul></li><li>Creates report</li><li>Make build fail<br><strong>Available tools</strong></li><li>JaCoCo</li><li>Clover</li><li>Cobertura<br><strong>Static Code Analysis</strong></li><li>Checks without execution</li><li>Checks number of rules</li><li>Rules apply to wide range of aspects</li><li>Popular tools are Checkstyle, FindBugs, and PMD</li></ul><p><strong>SonarQube</strong></p><ul><li>Quality management tool</li><li>An alternative</li><li>Aggregates different code analysis frameworks</li><li>Has own dashboards</li><li>User friendly web interface</li></ul><h2 id="Triggers-and-notifications"><a href="#Triggers-and-notifications" class="headerlink" title="Triggers and notifications"></a>Triggers and notifications</h2><p><strong>Triggers</strong></p><ul><li>Automatic action to start the build</li><li>Manay options to choose from</li><li>Three types:<ul><li>External</li><li>Polling SCM</li><li>Scheduled build<br><strong>External Trigger</strong></li></ul></li><li>Natural to understand</li><li>Starts the build after it’s called by the notifier</li></ul><p>GitHub -&gt; trigger-&gt; Jenkins</p><p><strong>Polling SCM</strong></p><ul><li>Periodically calls Github</li><li>Sound counter-intuitive<br>Polling SCM trigger is less intuitive.<br><img src="https://i.imgur.com/s6L2bbo.png" alt="Polling SCM figure"><br>Jenkins periodically calls GitHub and checks if there were any push to the repository. Then it starts the build.</li></ul><p><strong>Scheduled build</strong></p><ul><li>Jenkins runs the build periodically</li><li>No communication with any system</li><li>Implementation of scheduled build is same as polling SCM</li><li>Used for the commit pipeline</li></ul><p><strong>Notifications</strong></p><ul><li>Lot</li></ul><h2 id="Team-development-strategies"><a href="#Team-development-strategies" class="headerlink" title="Team development strategies"></a>Team development strategies</h2><p><strong>Development workflows</strong></p><ul><li>Put the code into the repostitory</li><li>Depends on many factors</li><li>Classify into three types:<ul><li><img src="https://i.imgur.com/WXIl9qU.png" alt="Trunk-based workflow"></li><li><img src="https://i.imgur.com/Oiury7J.png" alt="Branching workflow"></li><li><img src="https://i.imgur.com/5Uo3Ac9.png" alt="Forking workflow"><br><strong>Feature toggle</strong><br>Feature toggle is a technique that is an alternative to maintaining multiple source code branches such that the feature can be tested before it is completed and ready for use. It is used to disable the feature for users but enables it for developers while testing. Feature toggles are essentially variables used in conditional statements the simplest implementation of feature toggles are flags and the if statements.</li></ul></li></ul><p><strong>acceptance testing</strong><br>Acceptance testing is a test performed to determine if business requirements or contracts are met. </p><ul><li>Invovles black-box testing</li><li>Imply the acceptance of the software delivery</li><li>Also called UAT</li><li>Rely on manual steps</li><li>Reaonable to run them as programmed repeatable operations<br><strong>Artifact repository</strong><br>While the source control management stores the source code the artifact repository is dedicated for storing software binary artifacts, for example, compiled libraries or components later used to build a complete application.</li><li>Store binaries on a separate server due to:<ul><li>File size</li><li>Versions</li><li>Revision mapping</li><li>Packages</li><li>Access Control</li><li>Clients</li><li>Use cases<br><img src="https://i.imgur.com/5gDLWLc.png" alt="working"><br><strong>private docker registry</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p certs</span><br><span class="line"></span><br><span class="line">openssl req \\n  -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \\n  -x509 -days 365 -out certs/domain.crt</span><br><span class="line"></span><br><span class="line">docker run -d -p 5000:5000 --restart=always --name registry -v `pwd`/auth:/auth -e &quot;REGISTRY_AUTH=htpasswd&quot; -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/passwords -v `pwd`/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2</span><br><span class="line"></span><br><span class="line">docker tag ubuntu_with_python stanosaka/ubuntu_with_python:1</span><br><span class="line">docker push stanosaka/ubuntu_with_python:1</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><strong>Acceptance test in pipeline</strong><br><img src="https://i.imgur.com/06nJoTO.png" alt="process"></p><ol><li>The developer pushes a code change to Github.</li><li>Jenkins detects the change, triggers the build and checks out the current code.</li><li>Jenkins executes the commit phase and builds the Docker image</li><li>Jenkins pushes the image to Docker Registry</li><li>Jenkins runs the Docker container in the staging environment</li></ol><p><strong>Configuration management</strong></p><ul><li>Process of controlling configuration changes</li><li>Used to refer to the software and the hardware</li></ul><p>Application Configuration| Infrastructre Configuration<br>Decides how the system works| Server infrastructure and environment configuration<br>Expressed in the form of flags| Takes care of the deployment process<br><img src="https://i.imgur.com/L4k4cb0.png" alt="working"><br><strong>Traits</strong></p><ul><li>Automation</li><li>Version Control</li><li>Incremental changes</li><li>Server provisioning</li><li>Security</li><li>Simplicity</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Pipeline&lt;/p&gt;
&lt;p&gt;Represents a part of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Software delivery&lt;/li&gt;
&lt;li&gt;Quality assurance process&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benefits:&lt;/p&gt;
&lt;ul&gt;

      
    
    </summary>
    
    
      <category term="pipeline" scheme="http://223.95.78.227/tags/pipeline/"/>
    
  </entry>
  
  <entry>
    <title>anatomy-of-containers</title>
    <link href="http://223.95.78.227/2019/09/20/anatomy-of-containers/"/>
    <id>http://223.95.78.227/2019/09/20/anatomy-of-containers/</id>
    <published>2019-09-20T00:05:28.000Z</published>
    <updated>2019-09-22T13:58:47.702Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Anatomy-of-containers"><a href="#Anatomy-of-containers" class="headerlink" title="Anatomy of containers"></a>Anatomy of containers</h1><p>Containers are specially encapsulated and secured processes running on the host system.</p><p>Containers leverage a lot of features and primitives available in the Linux OS. The most important ones are</p><ul><li>namespaces</li><li>cgroups</li></ul><p>All processes running in containers share the same Linux kernel of the underlying host operating system. This is fundamentally different compared with VMs, as each VM contains its own full-blown operating system.</p><p>Difference:</p><table><thead><tr><th></th><th>Container</th><th>VMs</th></tr></thead><tbody><tr><td>startup times</td><td>milliseconds</td><td>several seconds</td></tr><tr><td>goal</td><td>ephemeral</td><td>long-living</td></tr></tbody></table><p><img src="https://i.imgur.com/3ReVPcO.png" alt="high level architecture of Docker"><br>On the lower part of the the preceding figure, we have the Linux operating system with its cgroups, namespaces, and layer capabilities as well as other functionality that we do not need to explicitly mention here. Then, there is an intermediary layer composed of <strong>containerd</strong> and <strong>runc</strong>. On top of all that now sits the Docker engine. The Docker engine offers a RESTful interface to the outside world that can be accessed by any tool, such as the Docker CLI, Docker for Mac, and Docker for Windows or Kubernetes to just name a few.</p><h2 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h2><p>A namespace is an abstraction of global resources such as filesystems, network access, process tree (also named PID namespace) or the system group IDs, and user IDs.<br><img src="https://i.imgur.com/GVSkK6z.png" alt="virtual FS"><br>The PID namespace is what keeps processes in one container from seeing or interacting with processes in another container. A process might have the apparent PID 1 inside a container, but if we examine it from the host system, it would have an ordinary PID, say 334:<br><img src="https://i.imgur.com/zhefA3P.png" alt="Process tree on a Docker host"></p><h2 id="Control-groups-cgroups"><a href="#Control-groups-cgroups" class="headerlink" title="Control groups (cgroups)"></a>Control groups (cgroups)</h2><p>Linux cgroups are used to limit, manage, and isolate resource usage of collections of processes running on a system.<br>Resources are CPU time, system memory, network bandwidth, or combinations of there resources, and so on.</p><p>Using cgroups, admins can limit the resources that containers can consume.With this, one can avoid, for example, the classical noisy neighbor problem, where a rogue process running in a container consumes all CPU time or reserves massive amounts of RAM and, as such, starves all the other processes running on the host, whether they’re containerized or not.</p><h2 id="Union-filesystem-UnionFS"><a href="#Union-filesystem-UnionFS" class="headerlink" title="Union filesystem (UnionFS)"></a>Union filesystem (UnionFS)</h2><p>The UnionFS forms the backbone of what is known as container images.<br>UnionFS is mainly used on Linux and allows files and directories of distinct filesystems to be overlaid and with it form a single coherent file system.In this context, the individual filesystems are called branches. Contents of directories that have the same path within the merged branches will be seen together in a single merged directory, within the new, virtual filesystem. When merging branches, the priority between the branches is specified. In that way, when two branches contain the same file, the one with the higher priority is seen in the final FS.</p><h2 id="Container-plumbing"><a href="#Container-plumbing" class="headerlink" title="Container plumbing"></a>Container plumbing</h2><p>The basement on top of which the Docker engine is built; we can also call it the container plumbing and is formed by the two component—runc and containerd.</p><h2 id="Runc"><a href="#Runc" class="headerlink" title="Runc"></a>Runc</h2><p>Runc is a lightweight, portable container runtime. It provides full support for Linux namespaces as well as native support for all security features available on Linux, such as SELinux, AppArmor, seccomp, and cgroups.</p><p>Runc is a tool for spawning and running containers according to the Open Container Initiative (OCI) specification.</p><h2 id="Containerd"><a href="#Containerd" class="headerlink" title="Containerd"></a>Containerd</h2><p>Runc is a low-level implementation of a container runtime; containerd builds on top of it, and adds higher-level features, such as image transfer and storage, container execution, and supervision, as well as network and storage attachments.</p><h1 id="Creating-and-managing-container-images"><a href="#Creating-and-managing-container-images" class="headerlink" title="Creating and managing container images"></a>Creating and managing container images</h1><h2 id="The-layered-filesystem"><a href="#The-layered-filesystem" class="headerlink" title="The layered filesystem"></a>The layered filesystem</h2><p><img src="https://i.imgur.com/Np4zxp1.jpg" alt="stack of layers"><br>The layers of a container image are all immutable. Immutable means that once generated, the layer cannot ever be changed. The only possible operation affecting the layer is the physical deletion of it.<br><img src="https://i.imgur.com/CkLp7N6.jpg" alt="A sample custom image based on Alpine and Nginx"><br>Each layer only contains the delta of changes in regard to the previous set of layers. The content of each layer is mapped to a special folder on the host system, which is usually a subfolder of /var/lib/docker/.</p><p>Since layers are immutable, they can be cached without ever becoming stale. This is a big advantage.</p><h2 id="The-writable-container-layer"><a href="#The-writable-container-layer" class="headerlink" title="The writable container layer"></a>The writable container layer</h2><p><img src="https://i.imgur.com/Gio85pZ.jpg" alt="the writable container layer"><br><img src="https://i.imgur.com/E80YSMK.jpg" alt="Multiple containers sharing the same image layers"><br>The container layer is marked as read/write. Another advantage of the immutability of image layers is that they can be shared among many containers created from this image. All that is needed is a thin, writable container layer for each container.</p><p>This technique, of course, results in a tremendous reduction of resources that are consumed. Furthermore, this helps to decrease the loading time of a container since only a thin container layer has to be created once the image layers have been loaded into memory, which only happens for the first container.</p><h2 id="Copy-on-write"><a href="#Copy-on-write" class="headerlink" title="Copy-on-write"></a>Copy-on-write</h2><p>Docker uses the copy-on-write technique when dealing with images. Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a layer uses a file or folder that is available in one of the low-lying layers, then it just uses it. If, on the other hand, a layer wants to modify, say, a file from a low-lying layer, then it first copies this file up to the target layer and then modifies it.<br><img src="https://i.imgur.com/hTj3sG0.jpg" alt="copy-on-write"><br>The second layer wants to modify <strong>File 2</strong>, which is present in the base layer. Thus, it copied it up and then modified it. Now, let’s say that we’re sitting in the top layer of the preceding figure. This layer will use <strong>File 1</strong> from the base layer and <strong>File 2</strong> and <strong>File 3</strong> from the second layer.</p><h2 id="Creating-images"><a href="#Creating-images" class="headerlink" title="Creating images"></a>Creating images</h2><p>Three ways to create a new container image on your system.</p><ol><li><p>Interactive image creation<br>start with a base image that we want to use as a template and run a container of it interactively. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">$ docker container run -it --name sample alpine /bin/sh</span><br><span class="line"></span><br><span class="line">#By default, the alpine container does not have the ping tool installed. Let&apos;s assume we want to create a new custom image that has ping installed. </span><br><span class="line"></span><br><span class="line"># apk update &amp;&amp; apk add iputils</span><br><span class="line"> # ping 127.0.0.1</span><br><span class="line">PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.</span><br><span class="line">64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.060 ms</span><br><span class="line">64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.054 ms</span><br><span class="line">64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.059 ms</span><br><span class="line">^C</span><br><span class="line">$ docker container ls -a | grep sample</span><br><span class="line">#If we want to see what has changed in our container in relation to the base image</span><br><span class="line">$ docker container diff sample</span><br><span class="line">C /var</span><br><span class="line">C /var/cache</span><br><span class="line">C /var/cache/apk</span><br><span class="line">A /var/cache/apk/APKINDEX.00740ba1.tar.gz</span><br><span class="line">A /var/cache/apk/APKINDEX.d8b2a6f4.tar.gz</span><br><span class="line">C /bin</span><br><span class="line">C /bin/ping</span><br><span class="line">C /bin/ping6</span><br><span class="line">A /bin/traceroute6</span><br><span class="line">C /lib</span><br><span class="line">C /lib/apk</span><br><span class="line">C /lib/apk/db</span><br><span class="line">C /lib/apk/db/triggers</span><br><span class="line">C /lib/apk/db/installed</span><br><span class="line">C /lib/apk/db/scripts.tar</span><br><span class="line">C /usr</span><br><span class="line">C /usr/sbin</span><br><span class="line">C /usr/sbin/arping</span><br><span class="line">A /usr/sbin/clockdiff</span><br><span class="line">A /usr/sbin/rarpd</span><br><span class="line">A /usr/sbin/setcap</span><br><span class="line">A /usr/sbin/tracepath</span><br><span class="line">A /usr/sbin/tracepath6</span><br><span class="line">A /usr/sbin/ninfod</span><br><span class="line">A /usr/sbin/getcap</span><br><span class="line">A /usr/sbin/tftpd</span><br><span class="line">A /usr/sbin/getpcaps</span><br><span class="line">A /usr/sbin/ipg</span><br><span class="line">A /usr/sbin/rdisc</span><br><span class="line">A /usr/sbin/capsh</span><br><span class="line">C /usr/lib</span><br><span class="line">A /usr/lib/libcap.so.2</span><br><span class="line">A /usr/lib/libcap.so.2.27</span><br><span class="line">C /etc</span><br><span class="line">C /etc/apk</span><br><span class="line">C /etc/apk/world</span><br><span class="line">C /root</span><br><span class="line">A /root/.ash_history</span><br><span class="line"></span><br><span class="line">#In the preceding list, A stands for added, and C for changed. If we had any deleted files, then those would be prefixed with D.</span><br><span class="line">#use the docker container commit command to persist our modifications and create a new image from them:</span><br><span class="line">$ docker container commit sample my-alpine</span><br><span class="line">sha256:31da02222ee7102dd755692bd498c6b170f11ffeb81ca700a4d12c0e5de9b913</span><br><span class="line"></span><br><span class="line">## If we want to see how our custom image has been built</span><br><span class="line">$ docker image history my-alpine</span><br><span class="line">IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT</span><br><span class="line">31da02222ee7        38 seconds ago      /bin/sh                                         1.76MB</span><br><span class="line">961769676411        4 weeks ago         /bin/sh -c #(nop)  CMD [&quot;/bin/sh&quot;]              0B</span><br><span class="line">&lt;missing&gt;           4 weeks ago         /bin/sh -c #(nop) ADD file:fe64057fbb83dccb9…   5.58MB</span><br><span class="line">#The first layer in the preceding list is the one that we just created by adding the iputils package.</span><br></pre></td></tr></table></figure></li><li><p>Using Dockerfiles<br>The Dockerfile is a text file that is usually literally called Dockerfile. It contains instructions on how to build a custom container image. It is a declarative way of building images.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM python:2.7</span><br><span class="line">RUN mkdir -p /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY ./requirements.txt /app/</span><br><span class="line">RUN pip install -r requirements.txt</span><br><span class="line">CMD [&quot;python&quot;, &quot;main.py&quot;]</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://i.imgur.com/NMD2UG5.jpg" alt="the relation of Dockerfile and layers in an image"></p><p><strong>The FROM keyword</strong><br>Every DOckerfile starts with the FROM keyword. With it, we define which base image we want to start building our custom image from. If build starting with CentOS 7:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:7</span><br></pre></td></tr></table></figure></p><p>If we really want to start from scratch:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FROM scratch</span><br></pre></td></tr></table></figure></p><p>FROM scratch is a no-op in the Dockerfile, and as such does not generate a layer in the resulting container image.</p><p><strong>The RUN keyword</strong><br>The argument for RUN is any valid Linux command, such as the following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RUN yum install -y wget</span><br></pre></td></tr></table></figure></p><p>If we use more than one line, we need to put a backslash () at the end of the lines to indicate to the shell that the command continues on the next line.<br><strong>The COPY and ADD keywords</strong><br>add some content to an existing base image to make it a custom image. Most of the time, these are a few source files of, say, a web application or a few binaries of a compiled application.</p><p>These two keywords are used to copy files and folders from the host into the image that we’re building. The two keywords are very similar, with the exception that the ADD keyword also lets us copy and unpack TAR files, as well as provide a URL as a source for the files and folders to copy.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">COPY . /app</span><br><span class="line">COPY ./web /app/web</span><br><span class="line">COPY sample.txt /data/my-sample.txt</span><br><span class="line">ADD sample.tar /app/bin/</span><br><span class="line">ADD http://example.com/sample.txt /data/</span><br></pre></td></tr></table></figure></p><p>In the preceding lines of code:</p><ul><li>The first line copies all files and folders from the current directory recursively to the /app folder inside the container image</li><li>The second line copies everything in the web subfolder to the target folder, /app/web</li><li>The third line copies a single file, sample.txt, into the target folder, /data, and at the same time, renames it to my-sample.txt</li><li>The fourth statement unpacks the sample.tar file into the target folder, /app/bin</li><li>Finally, the last statement copies the remote file, sample.txt, into the target file, /data</li></ul><p>Wildcards are allowed in the source path. For example, the following statement copies all files starting with sample to the mydir folder inside the image:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COPY ./sample* /mydir/</span><br></pre></td></tr></table></figure></p><p>From a security perspective, it is important to know that by default, all files and folders inside the image will have a user ID (UID) and a group ID (GID) of 0. The good thing is that for both ADD and COPY, we can change the ownership that the files will have inside the image using the optional –chown flag, as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ADD --chown=11:22 ./data/files* /app/data/</span><br></pre></td></tr></table></figure></p><p>The preceding statement will copy all files starting with the name web and put them into the /app/data folder in the image, and at the same time assign user 11 and group 22 to these files.</p><p>Instead of numbers, one could also use names for the user and group, but then these entities would have to be already defined in the root filesystem of the image at /etc/passwd and /etc/group respectively, otherwise the build of the image would fail.</p><p><strong>The WORKDIR keyword</strong><br>The WORKDIR keyword defines the working directory or context that is used when a container is run from our custom image. So, if I want to set the context to the /app/bin folder inside the image, my expression in the Dockerfile would have to look as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WORKDIR /app/bin</span><br></pre></td></tr></table></figure></p><p>All activity that happens inside the image after the preceding line will use this directory as the working directory. It is very important to note that the following two snippets from a Dockerfile are not the same:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RUN cd /app/bin</span><br><span class="line">RUN touch sample.txt</span><br></pre></td></tr></table></figure></p><p>Compare the preceding code with the following code:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WORKDIR /app/bin</span><br><span class="line">RUN touch sample.txt</span><br></pre></td></tr></table></figure></p><p>The former will create the file in the root of the image filesystem, while the latter will create the file at the expected location in the /app/bin folder. Only the WORKDIR keyword sets the context across the layers of the image. The cd command alone is not persisted across layers.</p><p><strong>The CMD and ENTRYPOINT keywords</strong><br>While all other keywords defined for a Dockerfile are executed at the time the image is built by the Docker builder, these two are actually definitions of what will happen when a container is started from the image we define. When the container runtime starts a container, it needs to know what the process or application will be that has to run inside this container. That is exactly what CMD and ENTRYPOINT are used for—to tell Docker what the start process is and how to start that process.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ping 8.8.8.8 -c 3</span><br></pre></td></tr></table></figure><p>ping is the command and 8.8.8.8 -c 3 are the parameters to this command.</p><p>ENTRYPOINT is used to define the command of the expression while CMD is used to define the parameters for the command. Thus, a Dockerfile using alpine as the base image and defining ping as the process to run in the container could look as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest</span><br><span class="line">ENTRYPOINT [&quot;ping&quot;]</span><br><span class="line">CMD [&quot;8.8.8.8&quot;, &quot;-c&quot;, &quot;3&quot;]</span><br></pre></td></tr></table></figure></p><p>For both ENTRYPOINT and CMD, the values are formatted as a JSON array of strings, where the individual items correspond to the tokens of the expression that are separated by whitespace. This the preferred way of defining CMD and ENTRYPOINT. It is also called the exec form.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ docker image build -t pinger .</span><br><span class="line">$ docker container run --rm -it pinger</span><br><span class="line">PING 8.8.8.8 (8.8.8.8): 56 data bytes</span><br><span class="line">64 bytes from 8.8.8.8: seq=0 ttl=56 time=5.612 ms</span><br><span class="line">64 bytes from 8.8.8.8: seq=1 ttl=56 time=4.389 ms</span><br><span class="line">64 bytes from 8.8.8.8: seq=2 ttl=56 time=6.592 ms</span><br><span class="line">#The beauty of this is that I can now override the CMD part that I have defined in the Dockerfile</span><br><span class="line">#This will now cause the container to ping the loopback for 5 seconds.</span><br><span class="line"></span><br><span class="line">$ docker container run --rm -it pinger -w 5 127.0.0.1</span><br><span class="line">PING 127.0.0.1 (127.0.0.1): 56 data bytes</span><br><span class="line">64 bytes from 127.0.0.1: seq=0 ttl=64 time=0.077 ms</span><br><span class="line">64 bytes from 127.0.0.1: seq=1 ttl=64 time=0.087 ms</span><br><span class="line">64 bytes from 127.0.0.1: seq=2 ttl=64 time=0.090 ms</span><br><span class="line">64 bytes from 127.0.0.1: seq=3 ttl=64 time=0.082 ms</span><br><span class="line">64 bytes from 127.0.0.1: seq=4 ttl=64 time=0.081 ms</span><br><span class="line"></span><br><span class="line">#If we want to override what&apos;s defined in the ENTRYPOINT in the Dockerfile, we need to use the --entrypoint parameter in the docker container run expression.</span><br><span class="line">docker container run --rm -it --entrypoint /bin/sh pinger</span><br><span class="line">/ # exit</span><br><span class="line"></span><br><span class="line">FROM alpine:latest</span><br><span class="line">CMD wget -O - http://www.google.com</span><br><span class="line"></span><br><span class="line">## If you leave ENTRYPOINT undefined, then it will have the default value of /bin/sh -c, and whatever is the value of CMD will be passed as a string to the shell command. The preceding definition would thereby result in entering following process to run inside the container:</span><br><span class="line">/bin/sh -c &quot;wget -O - http://www.google.com&quot;</span><br><span class="line"></span><br><span class="line">Consequently, /bin/sh is the main process running inside the container, and it will start a new child process to run the wget utility.</span><br></pre></td></tr></table></figure></p><p><img src="https://i.imgur.com/98sfbpv.jpg" alt="the image build process visualized"></p><h2 id="Multistep-builds"><a href="#Multistep-builds" class="headerlink" title="Multistep builds"></a>Multistep builds</h2><p>With a size of 176 MB, the resulting image is way too big. In the end, it is just a Hello World application. The reason for it being so big is that the image not only contains the Hello World binary, but also all the tools to compile and link the application from the source code. But this is really not desirable when running the application, say, in production. Ideally, we only want to have the resulting binary in the image and not a whole SDK.</p><p>It is precisely for this reason that we should define Dockerfiles as multistage. We have some stages that are used to build the final artifacts and then a final stage where we use the minimal necessary base image and copy the artifacts into it. This results in very small images. Have a look at this revised Dockerfile:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:3.7 AS build</span><br><span class="line">RUN apk update &amp;&amp; apk add --update alpine-sdk</span><br><span class="line">RUN mkdir /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY . /app</span><br><span class="line">RUN mkdir bin</span><br><span class="line">RUN gcc hello.c -o bin/hello</span><br><span class="line"></span><br><span class="line">FROM alpine:3.7</span><br><span class="line">COPY --from=build /app/bin/hello /app/hello</span><br><span class="line">CMD /app/hello</span><br></pre></td></tr></table></figure><p>Here, we have a first stage with an alias build that is used to compile the application, and then the second stage uses the same base image alpine:3.7, but does not install the SDK, and only copies the binary from the build stage, using the –from parameter, into this final image.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker image ls |grep hello-world</span><br><span class="line">hello-world-small                                              latest                                     46bb8c275fda        About a minute ago   4.22MB</span><br><span class="line">hello-world                                                    latest                                     75339d9c269f        7 minutes ago        178MB</span><br></pre></td></tr></table></figure></p><p>We have been able to reduce the size from 178 MB down to 4 MB. This is reduction in size by a factor of 40. A smaller image size has many advantages, such as a smaller attack surface area for hackers, reduced memory and disk consumption, faster startup times of the corresponding containers, and a reduction of the bandwidth needed to download the image from a registry, such as Docker Hub.</p><h2 id="Dockerfile-best-practices"><a href="#Dockerfile-best-practices" class="headerlink" title="Dockerfile best practices"></a>Dockerfile best practices</h2><ul><li>First and foremost, we need to consider that containers are meant to be ephemeral. By ephemeral, we mean that a container can be stopped and destroyed and a new one built and put in place with an absolute minimum of setup and configuration. That means that we should try hard to keep the time that is needed to initialize the application running inside the container at a minimum, as well as the time needed to terminate or clean up the application.</li><li>we should order the individual commands in the Dockerfile so that we leverage caching as much as possible. Building a layer of an image can take a considerable amount of time, sometimes many seconds or even minutes. While developing an application, we will have to build the container image for our application multiple times. We want to keep the build times at a minimum.</li></ul><blockquote><p>When we’re rebuilding a previously built image, the only layers that are rebuilt are the ones that have changed, but if one layer needs to be rebuilt, all subsequent layers also need to be rebuilt. This is very important to remember. Consider the following example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">FROM node:9.4</span><br><span class="line">RUN mkdir -p /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY . /app</span><br><span class="line">RUN npm install</span><br><span class="line">CMD [&quot;npm&quot;, &quot;start&quot;]</span><br></pre></td></tr></table></figure></p></blockquote><p>In this example, the npm install command on line five of the Dockerfile usually takes the longest. A classical Node.js application has many external dependencies, and those are all downloaded and installed in this step. This can take minutes until it is done. Therefore, we want to avoid running npm install each time we rebuild the image, but a developer changes their source code all the time during development of the application. That means that line four, the result of the COPY command, changes all the time and this layer has to be rebuilt each time. But as we discussed previously, that also means that all subsequent layers have to be rebuilt, which in this case includes the npm install command. To avoid this, we can slightly modify the Dockerfile and have the following:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM node:9.4</span><br><span class="line">RUN mkdir -p /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">COPY package.json /app/</span><br><span class="line">RUN npm install</span><br><span class="line">COPY . /app</span><br><span class="line">CMD [&quot;npm&quot;, &quot;start&quot;]</span><br></pre></td></tr></table></figure></p><p>What we have done here is that, on line four, we only copy the single file that the npm install command needs as a source, which is the package.json file. This file rarely changes in a typical development process. As a consequence, the npm install command also has to be executed only when the package.json file changes. All the remaining, frequently changed content is added to the image after the npm install command.</p><ul><li>A further best practice is to keep the number of layers that make up your image relatively small. The more layers an image has, the more the graph driver needs to work to consolidate the layers into a single root filesystem for the corresponding container. Of course, this takes time, and thus the fewer layers an image has, the faster the startup time for the container can be.<blockquote><p>The easiest way to reduce the number of layers is to combine multiple individual RUN commands into a single one—for example, say that we had the following in a Dockerfile:</p></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN apt-get update</span><br><span class="line">RUN apt-get install -y ca-certificates</span><br><span class="line">RUN rm -rf /var/lib/apt/lists/*</span><br></pre></td></tr></table></figure><p>We could combine these into a single concatenated expression, as follows:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN apt-get update \</span><br><span class="line">   &amp;&amp; apt-get install -y ca-certificates \</span><br><span class="line">   &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br></pre></td></tr></table></figure></p><p>The former will generate three layers in the resulting image, while the latter only creates a single layer.</p><p>The next three best practices all result in smaller images. Why is this important? Smaller images reduce the time and bandwidth needed to download the image from a registry. They also reduce the amount of disk space needed to store a copy locally on the Docker host and the memory needed to load the image. Finally, smaller images also means a smaller attack surface for hackers. Here are the best practices mentioned:</p><ol><li><p>The first best practice that helps to reduce the image size is to use a .dockerignore file. We want to avoid copying unnecessary files and folders into an image to keep it as lean as possible. A .dockerignore file works in exactly the same way as a .gitignore file, for those who are familiar with Git. In a .dockerignore file, we can configure patterns to exclude certain files or folders from being included in the context when building the image.</p></li><li><p>The next best practice is to avoid installing unnecessary packages into the filesystem of the image. Once again, this is to keep the image as lean as possible.</p></li><li><p>Last but not least, it is recommended that you use multistage builds so that the resulting image is as small as possible and only contains the absolute minimum needed to run your application or application service. </p></li></ol><h1 id="3-Saving-and-loading-images"><a href="#3-Saving-and-loading-images" class="headerlink" title="3. Saving and loading images"></a>3. Saving and loading images</h1><p>The third way to create a new container image is by importing or loading it from a file. A container image is nothing more than a tarball. To demonstrate this, we can use the docker image save command to export an existing image to a tarball:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker image save -o ./backup/my-alpine.tar my-alpine</span><br><span class="line">$ docker image load -i ./backup/my-alpine.tar</span><br></pre></td></tr></table></figure></p><h1 id="Running-in-production"><a href="#Running-in-production" class="headerlink" title="Running in production"></a>Running in production</h1><p><strong>Blue-green deployments</strong><br>In blue-green deployments, the current version of the application service, called blue, handles all the application traffic. We then install the new version of the application service, called green, on the production system. The new service is not yet wired with the rest of the application.</p><p>Once green is installed, one can execute smoke tests against this new service and, if those succeed, the router can be configured to funnel all traffic that previously went to blue to the new service, green. The behavior of green is then observed closely and, if all success criteria are met, blue can be decommissioned. But if, for some reason, green shows some unexpected or unwanted behavior, the router can be reconfigured to return all traffic to blue. Green can then be removed and fixed, and a new blue-green deployment can be executed with the corrected version:<br><img src="https://i.imgur.com/bWC8uLg.png" alt="blue-green deployment"></p><p><strong>Canary releases</strong><br>Canary releases are releases where we have the current version of the application service and the new version installed on the system in parallel. As such, they resemble blue-green deployments. At first, all traffic is still routed through the current version. We then configure a router so that it funnels a small percentage, say 1%, of the overall traffic to the new version of the application service. The behavior of the new service is then monitored closely to find out whether or not it works as expected. If all the criteria for success are met, then the router is configured to funnel more traffic, say 5% this time, through the new service. Again, the behavior of the new service is closely monitored and, if it is successful, more and more traffic is routed to it until we reach 100%. Once all traffic is routed to the new service and it has been stable for some time, the old version of the service can be decommissioned.</p><h1 id="System-management"><a href="#System-management" class="headerlink" title="System management"></a>System management</h1><h2 id="Pruning-unused-resources"><a href="#Pruning-unused-resources" class="headerlink" title="Pruning unused resources"></a>Pruning unused resources</h2><p><strong>Listing resource consumption</strong><br>$ docker system df</p><h1 id="in-verbose-mode"><a href="#in-verbose-mode" class="headerlink" title="in verbose mode"></a>in verbose mode</h1><p>$ docker system df -v<br><strong>Pruning containers</strong></p><h1 id="regain-unused-system-resources"><a href="#regain-unused-system-resources" class="headerlink" title="regain unused system resources"></a>regain unused system resources</h1><p>$ docker container prune # remove all containers from the system that are not in running status<br>$ docker container prune -f # skip confirmation step</p><h1 id="remove-all-containers-from-system-even-the-running-ones"><a href="#remove-all-containers-from-system-even-the-running-ones" class="headerlink" title="remove all containers from system, even the running ones"></a>remove all containers from system, even the running ones</h1><p>$ docker container rm -f $(docker container ls -aq)<br><strong>Pruning images</strong><br>$ docker image prune</p><h1 id="avoid-Docker-asking-for-a-confirmation"><a href="#avoid-Docker-asking-for-a-confirmation" class="headerlink" title="avoid Docker asking for a confirmation"></a>avoid Docker asking for a confirmation</h1><p>$ docker image prune -f</p><h1 id="Sometimes-we-do-not-just-want-to-remove-orphaned-image-layers-but-all-images-that-are-not-currently-in-use-on-our-system-For-this-we-can-use-the-a-or-–all-flag"><a href="#Sometimes-we-do-not-just-want-to-remove-orphaned-image-layers-but-all-images-that-are-not-currently-in-use-on-our-system-For-this-we-can-use-the-a-or-–all-flag" class="headerlink" title="Sometimes we do not just want to remove orphaned image layers but all images that are not currently in use on our system. For this, we can use the -a (or –all) flag:"></a>Sometimes we do not just want to remove orphaned image layers but all images that are not currently in use on our system. For this, we can use the -a (or –all) flag:</h1><p>$ docker image prune –force –all<br><strong>Pruning volumes</strong><br>$ docker volume prune<br>A useful flag when pruning volumes is the -f or –filter flag which allows us to specify the set of volumes which we’re considering for pruning<br>$ docker volume prune –filter ‘label=demo’<br><strong>Pruning network</strong><br>will remove the networks on which currently no container or service is attached.<br>$ docker network prune<br><strong>Pruning everything</strong><br>$ docker system prune</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Anatomy-of-containers&quot;&gt;&lt;a href=&quot;#Anatomy-of-containers&quot; class=&quot;headerlink&quot; title=&quot;Anatomy of containers&quot;&gt;&lt;/a&gt;Anatomy of containers&lt;/
      
    
    </summary>
    
    
      <category term="docker" scheme="http://223.95.78.227/tags/docker/"/>
    
      <category term="containers" scheme="http://223.95.78.227/tags/containers/"/>
    
  </entry>
  
  <entry>
    <title>apache-cloudstack</title>
    <link href="http://223.95.78.227/2019/09/19/apache-cloudstack/"/>
    <id>http://223.95.78.227/2019/09/19/apache-cloudstack/</id>
    <published>2019-09-19T12:56:11.000Z</published>
    <updated>2019-09-20T12:53:19.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="What-is-Apache-CloudStack"><a href="#What-is-Apache-CloudStack" class="headerlink" title="What is Apache CloudStack"></a>What is Apache CloudStack</h1><ul><li>Top level Apache Software Foundation project for cloud computing</li><li>Cloud computing framework to deploy IaaS cloud- private, public or hybrid</li><li>Supports all popular hypervisors for virtualization</li><li>Written in Java. Provides APIs and web GUI for management and administration</li></ul><h2 id="Key-features"><a href="#Key-features" class="headerlink" title="Key features"></a>Key features</h2><ul><li>Rich management interface</li><li>Powerful API</li><li>Dynamic workload management</li><li>Secure, configurable and extensible architecture<br><img src="https://i.imgur.com/J7Jm1XC.png" alt="architecuture"></li></ul><h2 id="Key-components-terminology"><a href="#Key-components-terminology" class="headerlink" title="Key components/terminology"></a>Key components/terminology</h2><ul><li>Region<ul><li>Largest Organizational Unit with a CloudStack Deployment</li><li>Consists of one or more zones</li></ul></li><li>Zone<ul><li>Consists of one or more pods</li><li>Contains one or more primary storage servers</li><li>Consists of a secondary storage</li></ul></li><li>Pod<ul><li>Contains one or more clusters</li><li>Contains primary storage servers</li></ul></li><li>Cluster<ul><li>Consists of a group of identical hosts running a common hypervisor</li><li>Contins one or more primary storage servers</li></ul></li><li>Host<ul><li>Smallest orgnizational unit within a Cloustack deployment</li><li>Has hypervisor to manage the guest VMs</li><li>Provides the computing resources that run guest VMs.</li></ul></li><li>Primary storage<br>iSCSI; NFS; Ceph; Gluster;  Local FileSystem</li><li>Secondary storage<br>Templates;  ISO images;   Disk volume; snapshots</li></ul><hr><p>Recap</p><ul><li>Cloud computing is more efficient than traditional IT infrastructure deployment and management</li><li>Cloud can be deployed in public, private and hybrid configurations</li><li>Cloud services can be provided as IaaS, PaaS or SaaS.</li><li>Apache CloudStack is an open source IaaS cloud platform</li><li>Apache CloudStack si very robust, extensible and open platform</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;What-is-Apache-CloudStack&quot;&gt;&lt;a href=&quot;#What-is-Apache-CloudStack&quot; class=&quot;headerlink&quot; title=&quot;What is Apache CloudStack&quot;&gt;&lt;/a&gt;What is Apa
      
    
    </summary>
    
    
      <category term="cloudstack" scheme="http://223.95.78.227/tags/cloudstack/"/>
    
  </entry>
  
  <entry>
    <title>networking</title>
    <link href="http://223.95.78.227/2019/09/12/networking/"/>
    <id>http://223.95.78.227/2019/09/12/networking/</id>
    <published>2019-09-12T08:37:21.000Z</published>
    <updated>2020-03-03T05:34:06.627Z</updated>
    
    <content type="html"><![CDATA[<h1 id="IP-subnetting"><a href="#IP-subnetting" class="headerlink" title="IP subnetting"></a>IP subnetting</h1><p><strong>Subnetting part 1</strong><br>IP address</p><ul><li>Subnet address </li><li>1st Host address </li><li>Last Host address </li><li>Broadcast address </li></ul><p>Two methods</p><ul><li>Method 1 - Binary Method</li><li>Method 2 - Quick Method</li></ul><p><img src="https://i.imgur.com/GFUUIuH.png" alt="Typical example"></p><ul><li><p>What IP address would router1 be configured with if it is to use the first ip address in the same subnet as PC1?</p></li><li><p>What broadcast address is use by PC1?</p></li><li><p>What ip address would Router1 be configured with if it is to use the last IP address in the same Subnet as PC1?</p></li><li><p>What subnet is PC1 part of?</p></li></ul><p><strong>Binary Rules</strong></p><p>Network/subnet address </p><ul><li>Fill the host portion of an address with binary 0’s </li></ul><p>Broadcast address </p><ul><li>Fill the host portion of an address with binary 1’s </li></ul><p>First Host</p><ul><li>Fill the host portion of an address with binary 0’s except for the last bit which is set to binary 1</li></ul><p>Last Host</p><ul><li>Fill the host portion of an address with binary 1’s except for the last bit which is set to binary 0</li></ul><p>3rd example<br>172.16.129.1/17</p><p>172.16.1|000 0001.0000 0001<br>subnet = 172.16.1000 0000.0000 0000=172.16.128.0<br>Broadcast= 172.16.1111 1111.1111 1111=172.16.255.255<br>1st host=172.16.128.1<br>last host=172.16.255.254</p><p><strong>Shortcut table</strong><br>128|64|32|16|8|4|2|1<br>128|192|224|240|248|252|254|255</p><p>Tip: the first line shows the decimal values for the binary numbers of an octet<br>10000000=128   00010000=16</p><p>Second row, subtract the value in the frist row from 256<br>256-128=128    256-32=224   256-64=192  256-1=255</p><p><strong>Binary odometer:(0-255)</strong><br>10.1.1.254+1=10.1.1.255<br>10.1.1.255+1=10.1.2.0<br>10.1.2.0+1=10.1.2.1<br>or in reverse:<br>10.1.2.0-1=10.1.1.255</p><p>Broadcast address = Next Network -1<br>First host = subnet +1<br>last host = broadcast -1<br>What subnet is this host on?<br>What is a last host?</p><p><strong>Subnetting part 2</strong><br>creat multiple subnets</p><p>Subnet this network into at least 10 subnets<br>Subnet this network into subnets each having 10 host</p><h1 id="Azure-VMs-networking"><a href="#Azure-VMs-networking" class="headerlink" title="Azure VMs networking"></a>Azure VMs networking</h1><p>Priority: the lower the number, the higher the priority.</p><h1 id="ipcalc-in-linux"><a href="#ipcalc-in-linux" class="headerlink" title="ipcalc in linux"></a>ipcalc in linux</h1><p>ipcalc 13.75.0.0/16<br>Address:   13.75.0.0            00001101.01001011. 00000000.00000000<br>Netmask:   255.255.0.0 = 16     11111111.11111111. 00000000.00000000<br>Wildcard:  0.0.255.255          00000000.00000000. 11111111.11111111<br>=&gt;<br>Network:   13.75.0.0/16         00001101.01001011. 00000000.00000000<br>HostMin:   13.75.0.1            00001101.01001011. 00000000.00000001<br>HostMax:   13.75.255.254        00001101.01001011. 11111111.11111110<br>Broadcast: 13.75.255.255        00001101.01001011. 11111111.11111111<br>Hosts/Net: 65534                 Class A</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;IP-subnetting&quot;&gt;&lt;a href=&quot;#IP-subnetting&quot; class=&quot;headerlink&quot; title=&quot;IP subnetting&quot;&gt;&lt;/a&gt;IP subnetting&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Subnetting part 1&lt;
      
    
    </summary>
    
    
      <category term="networking" scheme="http://223.95.78.227/tags/networking/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes-on-the-cloud</title>
    <link href="http://223.95.78.227/2019/08/31/kubernetes-on-the-cloud/"/>
    <id>http://223.95.78.227/2019/08/31/kubernetes-on-the-cloud/</id>
    <published>2019-08-31T03:47:20.688Z</published>
    <updated>2019-08-31T03:47:20.688Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-on-the-GKE"><a href="#Kubernetes-on-the-GKE" class="headerlink" title="Kubernetes on the GKE"></a>Kubernetes on the GKE</h1><p><strong>use cloud shell</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#setup zone</span><br><span class="line">gcloud config set compute/zone australia-southeast1-a</span><br><span class="line">#setup region</span><br><span class="line">gcloud config set compute/region australia-southeast1</span><br><span class="line"># create a single node cluster</span><br><span class="line">gcloud container clusters create my-first-cluster --num-nodes 1</span><br><span class="line"># see the list of clusters</span><br><span class="line">gcloud compute instances list</span><br><span class="line"># deploy a wordpress Docker container to our single node cluster</span><br><span class="line"># --image=tutum/wordpress: An out-fo-the-box image which includes everything you need</span><br><span class="line"># to run this site-including a MySQL database</span><br><span class="line">kubectl run wordpress --image=tutum/wordpress --port=80</span><br><span class="line"># see the status of pods running</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ kubectl get pods</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">wordpress-7fff795d48-kzbfr   1/1     Running   0          5m36s</span><br></pre></td></tr></table></figure></p><p>By default a pod is accessible to only other internal machines in the cluster<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#kubectl expose pod: Exposed the pod a a service so it can be accessed externally</span><br><span class="line">#--type=LoadBalancer: Creates an external IP that this pod can use to accept traffic</span><br><span class="line">kubectl expose pod wordpress-7fff795d48-kzbfr --name=wordpress --type=LoadBalancer</span><br><span class="line"># check </span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ kubectl describe svc wordpress</span><br><span class="line">Name:                     wordpress</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   pod-template-hash=7fff795d48</span><br><span class="line">                          run=wordpress</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 pod-template-hash=7fff795d48,run=wordpress</span><br><span class="line">Type:                     LoadBalancer</span><br><span class="line">IP:                       10.27.245.246</span><br><span class="line">LoadBalancer Ingress:     35.189.22.57</span><br><span class="line">Port:                     &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  31344/TCP</span><br><span class="line">Endpoints:                10.24.0.11:80</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                Age    From                Message</span><br><span class="line">  ----    ------                ----   ----                -------</span><br><span class="line">  Normal  EnsuringLoadBalancer  2m17s  service-controller  Ensuring load balancer</span><br><span class="line">  Normal  EnsuredLoadBalancer   81s    service-controller  Ensured load balancer</span><br></pre></td></tr></table></figure></p><p><strong>How Kubernetes Works</strong><br>Kubernetes: Orchestration technology- convert isolated containers running on different hardware into a cluster</p><p>Pod: Atomic unit of deployment in Kubernetes.</p><ul><li>Consists of 1 or more tightly coupled containers.</li><li>Pod runs on node, which is controlled by master.</li><li>Kubernetes only knows about pods</li><li>Cannot start container without a pod</li><li>Pod =&gt; Sandbox for 1 or more cntainers</li></ul><p><img src="https://i.imgur.com/m8uoP3H.png" alt="pods"><br>Kubernetes::Hadoop</p><ul><li>Docker Container Engine -&gt; Java Runtime</li><li>Docker containers -&gt; Jars</li><li>Kubernetes -&gt; Hadoop</li></ul><p><strong>Kubernetes for Orchestration</strong></p><ul><li>Fault-tolerance: Pod/Nod failures</li><li>Auto-scaling: More clients? More demand</li><li>Rollback: Advanced deployment options</li><li>Auto-healing: Crashed containers restart</li><li>Load-balancing: Distribute client requests</li><li>Isolation: Sandboxes so that containers don’t interfere</li></ul><p><strong>kubectl</strong><br><img src="https://i.imgur.com/GryAqTK.png" alt="kubectl"><br>Telling k8s what the desire state is.</p><p><strong>What does the k8s master do?</strong><br>Kubernetes Master</p><ul><li>One or more nodes designated as master</li><li>Several k8s processes run on master</li><li>Multi-master for high-availability</li></ul><p><strong>kube-apiserver</strong></p><ul><li>Communicates with user</li><li>RESTful API end-points</li><li>Manifest yaml files are accepted by apiserver<br><img src="https://i.imgur.com/GA0zKFs.png" alt="apiserver"><br><strong>etcd</strong><br>Cluster Store for Metadata</li><li>Metadata about spec and status of cluster</li><li>etcd is consistent and highly available key-value store</li><li>source-of-truth for cluster state<br><img src="https://i.imgur.com/kQQe478.png" alt="etcd"></li></ul><p><strong>kube-scheduler</strong></p><ul><li>Handle pod creation and management</li><li>Kube-scheduler match/assign nodes to pods</li><li>Complex-affinities, taints, tolerations,…<br><img src="https://i.imgur.com/erz1Rpw.png" alt="scheduler"></li></ul><p><strong>controller-manager</strong></p><ul><li>Different master processes</li><li>Actual state <-> Desired State</li><li>cloud-controller-manager</li><li>kube-controller-manager</li><li>Node controller</li><li>Replication controller</li><li>Route controller</li><li>Volume controller<br><img src="https://i.imgur.com/iQpH2WD.png" alt="controller"></li></ul><p><strong>Control Plane</strong></p><ul><li>Apiserver</li><li>etc</li><li>scheduler</li><li>controller-manager<ul><li>cloud-controller-manager</li><li>kube-controller-manager<br><img src="https://i.imgur.com/ql9VYxt.png" alt="control plane"></li></ul></li></ul><p><strong>What runs on each node of a cluster?</strong><br>Kubernetes Node (Minion)</p><ul><li>Kubelet<ul><li>Agent running on this node</li><li>Listen to k8s master</li><li>Port 10255</li></ul></li><li>Kube-proxy<ul><li>Needed because pod IP addresses are ephemeral</li><li>Networking- will make sense when we discuss Service objects</li></ul></li><li>Container engine<ul><li>Works with kubelet</li><li>Pulling images</li><li>Start/stop</li><li>Could be Docker or rkt<br><img src="https://i.imgur.com/lXOojJr.png" alt="Minion"><br><img src="https://i.imgur.com/dft7Zwy.png" alt="Pod Creation Request"></li></ul></li></ul><p><strong>What are Pods?</strong><br><img src="https://i.imgur.com/5qt1aB6.png" alt="pod creation yaml"><br>Multi-Container Pods</p><ul><li>Share access to memory space</li><li>Connect to each other using localhost</li><li>Share access to the same volumes (storage abstraction)</li><li>Same parameters such as configMaps</li><li>Tight coupling is dangerous</li><li>Once crashes, all crash</li></ul><p><strong>Use cases for multi-container Pod</strong></p><ul><li>Main container, “sidecar” supporting containers<br><img src="https://i.imgur.com/ollFuAy.png" alt="main container"></li><li>Proxies, Bridges, Adapters<br><img src="https://i.imgur.com/kan5QHv.png" alt="Proxies"></li></ul><p><strong>Anti-Patterns for multi-container pods</strong></p><ul><li>Avoid packing three-tier web app into 1 pod</li><li>Do not pack multiple similar containers  in the same pod for scaling</li><li>Use Deployments or ReplicaSet instead</li><li>Micro0services: Simple, independent components</li></ul><p><strong>Pods limitations</strong></p><ul><li>No auto-healing or scaling</li><li>Pod crashes? Must be handled at higher level<ul><li>ReplicaSet,Deployment,Service</li></ul></li><li>Ephemeral: Ip address are ephemeral<br><strong>Higer level k8s objects</strong></li><li>ReplicaSet, ReplicationController: Scaling and healing</li><li>Deployment: Versioning and rollback</li><li>Service: Static (non-ephemeral) IP and networking</li><li>Volume: Non-ephemeral storage<h2 id="How-do-master-nodes-communicate"><a href="#How-do-master-nodes-communicate" class="headerlink" title="How do master nodes communicate?"></a>How do master nodes communicate?</h2><strong>Cluster to master</strong></li><li>All cluster -&gt; master communication only with apiserver</li><li>HTTPS (443)</li><li>Relatively secure<br><strong>Master to cluster</strong></li><li>apiserver-&gt;kubelet<ul><li>Certicate not verified by default</li><li>Vulnerable to man-in-the-middle attacks</li><li>Don’t run on public network</li><li>To harden<ul><li>set -kubelet-certificate-authority</li><li>use SSH tunelling</li></ul></li></ul></li><li>apiserver-&gt;nodes/pods/services<ul><li>Not safe</li><li>Plain HTTP</li><li>Neither authenticated for encrypted</li><li>On public clouds, SSH tunnelling provided by cloud provider e.g. GCP<h2 id="Where-can-we-run-Kubernetes"><a href="#Where-can-we-run-Kubernetes" class="headerlink" title="Where can we run Kubernetes?"></a>Where can we run Kubernetes?</h2>Public Cloud: </li></ul></li><li>aws(kops)</li><li>azure </li><li>gcp(kubectl)<ul><li>GKE:Google Kubernetes Engine</li><li>Infra on GCE Virtual machines</li><li>GCE: Google Compute Engine (IaaS)<br>Bootstrap(kubeadm): On-prem, private cloud<br>Playgrounds:PWK(Browser-based,time-limited sessions), Minikube(Windows or Mac, Sets up VM on your machine)</li></ul></li></ul><h2 id="Can-K8s-be-used-in-a-hybird-multi-cloud-world"><a href="#Can-K8s-be-used-in-a-hybird-multi-cloud-world" class="headerlink" title="Can K8s be used in a hybird, multi-cloud world?"></a>Can K8s be used in a hybird, multi-cloud world?</h2><p>Hybrid=On-prem+Public Cloud</p><ul><li>On-premise: vare metal or VMs</li><li>Legacy infra, large on-prem datacenters</li><li>Medium-term importance</li></ul><p>Multi-Cloud</p><ul><li>More than 1 public cloud provider</li><li>Strategic reasons, vendor lock-in (Amazon buying Whole Foods)</li></ul><h2 id="Interacting-with-K8s"><a href="#Interacting-with-K8s" class="headerlink" title="Interacting with K8s"></a>Interacting with K8s</h2><p><strong>How do we work with k8s</strong></p><ul><li>kubectl<ul><li>Most common command line utility</li><li>Make POST requests to apiserver of control plane</li></ul></li><li>kubeadm<ul><li>Bootstrap cluster when not on cloud Kubernetes service</li><li>To create cluster out of individual infra nodes</li></ul></li><li>kubefed<ul><li>Administer federated clusters</li><li>Federated cluster-&gt; group of multiple clusters (multi-cloud,hybrid)</li></ul></li><li>kubelet</li><li>kube-proxy</li></ul><p><strong>Objects</strong></p><ul><li>K8s objects are persistent entities</li><li>Everything is an object…</li><li>Pod,ReplicaSet, Deployment, Node … all are objects</li><li>Send object specification (usually in .yaml or .json)</li></ul><p><strong>Three object management methods</strong></p><ul><li>Imperative commands<ul><li>No .yaml or config files</li><li>kubectl run …</li><li>kubectl expose …</li><li>kubectl autosacle …<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl run ningx --image nginx</span><br><span class="line">kubectl create deployment nginx --image nginx</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>No config file</p><p>Imperative: intent is in command</p><p>Pro:</p><ul><li>Simple<br>Cons:</li><li>No audit trail or review mechanism</li><li>Can’t reuse or use in template</li></ul><ul><li>Imperative object configuration<ul><li>kubectl + yaml or config files used</li><li>kubectl create -f config.yaml</li><li>kubectl replace -f config.yaml</li><li>kubectl delete -f config.yaml</li></ul></li></ul><p>Config file required<br>Still Imperative: intent is in command</p><p>Pros:</p><ul><li>Still simple</li><li>Robust - files checked into repo</li><li>One file for multiple operations</li></ul><ul><li>Declarative object configuration<ul><li>Only .yaml or config files used</li><li>kubectl apply -f config.yaml<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f configs/</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>Config files(s) are all that is required<br>Declarative not imperative</p><p>Pros:</p><ul><li>Most robust-review, repos, audit trails</li><li>K8S will automatically figure out intents</li><li><p>Can specify multiple files/directories recursively</p></li><li><p>Live object configuration: The live configuration values of an object, as observed by the Kubernetes cluster</p></li><li>Current object configuration files: The config file we are applying in the current command</li><li>List-applied object configuration file: The last config file what was applied to the object</li></ul><p><em>Don’t mix and match!</em><br><em>Declarative is preferred</em></p><p><strong>Merging Changes</strong></p><ul><li>Primitive fields<ul><li>String, init, boolean, images or replicas</li><li>Replace old state with Current object configuration file</li></ul></li><li>Map fields<ul><li>Merge old state with Current object configuration file</li></ul></li><li>List field<ul><li>Complex-varies by field</li></ul></li></ul><p><strong>The Pros and Cons of Declarative and Imperative object management</strong><br>Declarative</p><p>kubectl apply -f config.yaml</p><ul><li>Robust</li><li>Track in repos, review</li><li>Recursively apply to directories</li></ul><p>Imperative<br>kubectl run …<br>kubectl expose…<br>kubectl autosacle…</p><ul><li>Simple, intention is very clear</li><li>Preferred for deletion</li></ul><h2 id="Names-and-UIDs"><a href="#Names-and-UIDs" class="headerlink" title="Names and UIDs"></a>Names and UIDs</h2><p><strong>How are objects named?</strong></p><ul><li>Objects-persistent entities<ul><li>pods,replicasets,services,volumes,nodes,…</li><li>information maintained in etcd</li></ul></li><li>Identified using<ul><li>names: client-given</li><li>UIDs: system-generated<h2 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h2></li></ul></li><li>Divide physical cluster into multiple virtual clusters</li><li>Three pre-defined namespaces:<ul><li>default: if non specified</li><li>kube-system: for internal k8s objects</li><li>kub-public: auto-readable by all users</li></ul></li><li>Name scopes: names need to be unique only inside a namespace</li><li>Future version: namespace-&gt; common access control</li><li>Don’t use namespaces for versioning<ul><li>Just use labels instead</li></ul></li></ul><p><strong>Objects without namespaces</strong></p><ul><li>Nodes</li><li>PersistentVolumes</li><li>Namespace themselves</li></ul><h2 id="Labels"><a href="#Labels" class="headerlink" title="Labels"></a>Labels</h2><ul><li>key/value pairs attached to objects</li><li>metadata</li><li>need not be unique (same label to mutiple objects)</li><li>No sematics for k8s (meaningful only to humans)</li><li>Loose coupling via selectors</li><li>Can be added<ul><li>at creation time</li><li>after creation</li></ul></li><li>Multiple objects can have same label</li><li>Same label can’t repeat key though</li></ul><h2 id="Volumes"><a href="#Volumes" class="headerlink" title="Volumes"></a>Volumes</h2><ul><li>Permanence: Storage that lasts longer than lifetime of a pod<ul><li>a container inside pod (true for all volumes)</li><li>a pod (only true for persistent volumes)</li></ul></li><li>Shared State: multiple containers in a pod need to share state/files</li><li>Volumes: address both these needs</li><li>Volumes (in general): lifetime of abstraction=lifetime of pod<ul><li>Note that this is longer than lifetime of any container inside pod</li></ul></li><li>Persistent Volumes: lifetime of abstraction independent of pod lifetime<br><img src="https://i.imgur.com/0DJ10wq.png" alt="using volume"><br><strong>Types of Volumes</strong></li><li>awsElasticBlockStore</li><li>azureDisk</li><li>azureFile</li><li>gcePersistentDisk</li><li>Many non-cloud-specific volume types as well<br><strong>Important type of volumes</strong></li><li>configMap</li><li>emptyDir</li><li>gitRepo</li><li>secret</li><li>hostPath<h2 id="Lab-Volumes-and-the-exmptydir-volume"><a href="#Lab-Volumes-and-the-exmptydir-volume" class="headerlink" title="Lab: Volumes and the exmptydir volume"></a>Lab: Volumes and the exmptydir volume</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ cat pod-redis.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">   name: redis</span><br><span class="line">spec:</span><br><span class="line">   containers:</span><br><span class="line">   - name: redis</span><br><span class="line">     image: redis</span><br><span class="line">     volumeMounts:</span><br><span class="line">     - name: redis-storage</span><br><span class="line">       mountPath: /data/redis</span><br><span class="line">   volumes:</span><br><span class="line">   - name: redis-storage</span><br><span class="line">     emptyDir: &#123;&#125;</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod redis --watch        NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">redis   0/1     Pending   0          7m36s</span><br><span class="line">redis   0/1   Pending   0     7m36s</span><br><span class="line">redis   0/1   ContainerCreating   0     7m36s</span><br><span class="line">redis   1/1   Running   0     7m40s</span><br><span class="line">^Cstancloud9@cloudshell:~ (scenic-torch-250909)$ k exec -it redis -- /bin/bash</span><br><span class="line">root@redis:/data# cd /data/redis</span><br><span class="line">root@redis:/data/redis# echo Hello &gt; test-file</span><br><span class="line">root@redis:/data/redis# kill 1</span><br><span class="line">root@redis:/data/redis# command terminated with exit code 137</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod redis --watch</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">redis   1/1     Running   1          18m</span><br><span class="line">^Cstancloud9@cloudshell:~ (scenic-torch-250909)$ k exec -it redis -- /bin/bash</span><br><span class="line">root@redis:/data# ls /data/redis</span><br><span class="line">test-file</span><br></pre></td></tr></table></figure></li></ul><h2 id="Persistent-Volumes"><a href="#Persistent-Volumes" class="headerlink" title="Persistent Volumes"></a>Persistent Volumes</h2><ul><li>Low-level objects, like nodes</li><li>Two types of provisioning:<ul><li>Static: administrator pre-creates the volumes</li><li>Dynamic: containers need to file a PersistentVolumeClaim<br><img src="https://i.imgur.com/Yk4bu0J.png" alt="Cloud specific persistent volumes"><br><img src="https://i.imgur.com/KnjqWkU.png" alt="persisten volumes"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">gcloud compute disks create my-disk-1 --zone australia-southeast1-a</span><br><span class="line">gcloud compute disks list</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ cat volum-sample.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: k8s.gcr.io/test-webserver</span><br><span class="line">    name: test-container</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /test-pd</span><br><span class="line">      name: test-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test-volume</span><br><span class="line">    gcePersistentDisk:</span><br><span class="line">      pdName: my-disk-1</span><br><span class="line">      fsType: ext4</span><br><span class="line">k create -f volum-sample.yaml --validate=false</span><br><span class="line">pod/test-pd created</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod test-pd --watch</span><br><span class="line">NAME      READY   STATUS              RESTARTS   AGE</span><br><span class="line">test-pd   0/1     ContainerCreating   0          23s</span><br><span class="line">test-pd   1/1   Running   0     29s</span><br><span class="line">^Cstancloud9@cloudshell:~ (scenic-torch-250909)$ k describe pod test-pd</span><br><span class="line">Name:               test-pd</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               gke-my-first-cluster-default-pool-8ca613e9-w13m/10.152.0.2</span><br><span class="line">Start Time:         Sat, 31 Aug 2019 11:39:22 +1000</span><br><span class="line">Labels:             &lt;none&gt;</span><br><span class="line">Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container test-container</span><br><span class="line">Status:             Running</span><br><span class="line">IP:                 10.24.0.22</span><br><span class="line">Containers:</span><br><span class="line">  test-container:</span><br><span class="line">    Container ID:   docker://8b08b4cd144d256a078fe3c7b4031844e2c3355d854a711ec454611c0edea30b</span><br><span class="line">    Image:          k8s.gcr.io/test-webserver</span><br><span class="line">    Image ID:       docker-pullable://k8s.gcr.io/test-webserver@sha256:f63e365c13646f231ec4a16791c6133ddd7b80fcd1947f41ab193968e02b0745</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sat, 31 Aug 2019 11:39:50 +1000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        100m</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /test-pd from test-volume (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gw57h (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True</span><br><span class="line">  Ready             True</span><br><span class="line">  ContainersReady   True</span><br><span class="line">  PodScheduled      True</span><br><span class="line">Volumes:</span><br><span class="line">  test-volume:</span><br><span class="line">    Type:       GCEPersistentDisk (a Persistent Disk resource in Google Compute Engine)</span><br><span class="line">    PDName:     my-disk-1</span><br><span class="line">    FSType:     ext4</span><br><span class="line">    Partition:  0</span><br><span class="line">    ReadOnly:   false</span><br><span class="line">  default-token-gw57h:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-gw57h</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       Burstable</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                  Age    From                                                      Message</span><br><span class="line">  ----    ------                  ----   ----                                                      -------</span><br><span class="line">  Normal  Scheduled               2m15s  default-scheduler                                         Successfully assigned default/test-pd to gke-my-first-cluster-default-pool-8ca613e9-w13m</span><br><span class="line">  Normal  SuccessfulAttachVolume  2m9s   attachdetach-controller                                   AttachVolume.Attach succeeded for volume &quot;test-volume&quot;</span><br><span class="line">  Normal  Pulling                 109s   kubelet, gke-my-first-cluster-default-pool-8ca613e9-w13m  pulling image &quot;k8s.gcr.io/test-webserver&quot;</span><br><span class="line">  Normal  Pulled                  107s   kubelet, gke-my-first-cluster-default-pool-8ca613e9-w13m  Successfully pulled image &quot;k8s.gcr.io/test-webserver&quot;</span><br><span class="line">  Normal  Created                 107s   kubelet, gke-my-first-cluster-default-pool-8ca613e9-w13m  Created container</span><br><span class="line">  Normal  Started                 107s   kubelet, gke-my-first-cluster-default-pool-8ca613e9-w13m  Started container</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><strong>What are some important types of volumes?</strong></p><ul><li><p>emptyDir</p><ul><li>Not persistent</li><li>Created when pod is created on node</li><li>Initially empty</li><li>Share space/state across containers in same pod</li><li>Containers acan mount at different times</li><li>When pod removed/crashes, data lost</li><li>When container carshes data remains</li><li>Usecases: Scratch space, checkpointing…<br><img src="https://i.imgur.com/GGAyCl0.png" alt="emptyDir"></li></ul></li><li><p>hostPath<br><img src="https://i.imgur.com/pzfvzAb.png" alt="hostPath"></p><ul><li>Mount file/directory from node filesystem into pod</li><li>Uncommon- pods should be independent of nodes</li><li>Makes pod-node coupling tight</li><li>Usecases: Access docker internals, running cAdvisor</li><li>Block devices or sockets on host</li></ul></li><li><p>gitRepo<br><img src="https://i.imgur.com/EPHpRzS.png" alt="gitRepo"></p></li><li><p>configMap<br><img src="https://i.imgur.com/GkC7kjP.png" alt="conigMap"></p><ul><li>configMap volume mounts data from ConfigMap object</li><li>configMap objects define key-value pairs</li><li>configMap objects inject paramters into pods</li><li>Two main usecases:<ul><li>Providing config information for apps running inside pods</li><li>Specifying config information for control plane (controllers)</li></ul></li></ul></li><li><p>secret</p><ul><li>Pass ensitive inforamtion to pods</li><li>First create secret so it is stored in control plane<ul><li>kubectl create secret</li></ul></li><li>Mount that secret as a volume so that it is available inside pod</li><li>Secret is stored in RAM storage (not written to persistent disk)<h2 id="Lab-use-of-secrets-pass-information-to-pods"><a href="#Lab-use-of-secrets-pass-information-to-pods" class="headerlink" title="Lab: use of secrets pass information to pods"></a>Lab: use of secrets pass information to pods</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ cat secrets.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: test-secret</span><br><span class="line">data:</span><br><span class="line">  username: bXktYMxw</span><br><span class="line">  password: Mzk1MjqkdmRnN0pi</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ cat secrets-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: secret-test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: test-container</span><br><span class="line">      image: nginx</span><br><span class="line">      volumeMounts:</span><br><span class="line">         # name must match the volume name below</span><br><span class="line">         - name: secret-volume</span><br><span class="line">           mountPath: /etc/secret-volume</span><br><span class="line">  # The secret data is exposed to Containers in the Pod through a Volume.</span><br><span class="line">  volumes:</span><br><span class="line">    - name: secret-volume</span><br><span class="line">      secret:</span><br><span class="line">        secretName: test-secret</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod secret-test-pod</span><br><span class="line">NAME              READY   STATUS    RESTARTS   AGE</span><br><span class="line">secret-test-pod   1/1     Running   0          19m</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k exec -it secret-test-pod -- /bin/bash</span><br><span class="line">root@secret-test-pod:/# cd /etc/secret-volume/</span><br><span class="line">root@secret-test-pod:/etc/secret-volume# ls</span><br><span class="line">password  username</span><br></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Debugging"><a href="#Debugging" class="headerlink" title="Debugging"></a>Debugging</h1><h2 id="Insufficient-cpu"><a href="#Insufficient-cpu" class="headerlink" title="Insufficient cpu"></a>Insufficient cpu</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod secret-test-pod</span><br><span class="line">NAME              READY   STATUS    RESTARTS   AGE</span><br><span class="line">secret-test-pod   0/1     Pending   0          51s</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k describe pod secret-test-pod</span><br><span class="line">Name:               secret-test-pod</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line">Node:               &lt;none&gt;</span><br><span class="line">Labels:             &lt;none&gt;</span><br><span class="line">Annotations:        kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container test-container</span><br><span class="line">Status:             Pending</span><br><span class="line">IP:</span><br><span class="line">Containers:</span><br><span class="line">  test-container:</span><br><span class="line">    Image:      nginx</span><br><span class="line">    Port:       &lt;none&gt;</span><br><span class="line">    Host Port:  &lt;none&gt;</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        100m</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /etc/secret-volume from secret-volume (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gw57h (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status</span><br><span class="line">  PodScheduled   False</span><br><span class="line">Volumes:</span><br><span class="line">  secret-volume:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  test-secret</span><br><span class="line">    Optional:    false</span><br><span class="line">  default-token-gw57h:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-gw57h</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       Burstable</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                  From               Message</span><br><span class="line">  ----     ------            ----                 ----               -------</span><br><span class="line">  Warning  FailedScheduling  11s (x8 over 4m25s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu.</span><br><span class="line"></span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ gcloud container clusters resize my-first-cluster --num-nodes=3 --zone australia-southeast1-a</span><br><span class="line">Pool [default-pool] for [my-first-cluster] will be resized to 3.</span><br><span class="line"></span><br><span class="line">Do you want to continue (Y/n)?  Y</span><br><span class="line"></span><br><span class="line">Resizing my-first-cluster...done.</span><br><span class="line">Updated [https://container.googleapis.com/v1/projects/scenic-torch-250909/zones/australia-southeast1-a/clusters/my-first-cluster].</span><br><span class="line">stancloud9@cloudshell:~ (scenic-torch-250909)$ k get pod secret-test-pod</span><br><span class="line">NAME              READY   STATUS    RESTARTS   AGE</span><br><span class="line">secret-test-pod   1/1     Running   0          19m</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kubernetes-on-the-GKE&quot;&gt;&lt;a href=&quot;#Kubernetes-on-the-GKE&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes on the GKE&quot;&gt;&lt;/a&gt;Kubernetes on the GKE&lt;/
      
    
    </summary>
    
    
      <category term="k8s" scheme="http://223.95.78.227/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>effective devops with aws</title>
    <link href="http://223.95.78.227/2019/08/16/effective-devops-with-aws/"/>
    <id>http://223.95.78.227/2019/08/16/effective-devops-with-aws/</id>
    <published>2019-08-15T14:15:29.000Z</published>
    <updated>2019-08-21T09:47:54.738Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Code</strong></p><p><a href="https://github.com/stanosaka/ansible">source code ansible</a><br><a href="https://github.com/00root/helloworld">source code helloworld</a></p><h1 id="Continuous-Delivery"><a href="#Continuous-Delivery" class="headerlink" title="Continuous Delivery"></a>Continuous Delivery</h1><p><img src="https://i.imgur.com/QOYWSYb.png" alt="codepipline01"><br><img src="https://i.imgur.com/an0PPjD.png" alt="codepipline02"><br><img src="https://i.imgur.com/P7gdZBo.png" alt="Source"><br><img src="https://i.imgur.com/O6LNT3M.png" alt="Test"><br><img src="https://i.imgur.com/gD0EYZ7.png" alt="Deploy"><br><img src="https://i.imgur.com/uAnMDQk.png" alt="Approval"><br><img src="https://i.imgur.com/OoeILa2.png" alt="Production"><br><img src="https://i.imgur.com/Mz3RM0I.png" alt="Jenkins01"><br><img src="https://i.imgur.com/88C8zGu.png" alt="Jenkins02"><br><img src="https://i.imgur.com/7YXnt5L.png" alt="Jenkins03"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry http://registry.npmjs.org/ </span><br><span class="line">npm install</span><br><span class="line">./node_modules/mocha/bin/mocha</span><br></pre></td></tr></table></figure></p><p><strong>Creating the new cloudformation stack for production</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">aws cloudformation create-stack --capabilities CAPABILITY_IAM --stack-name helloworld-production --template-body file://nodeserver-cf.template --parameters ParameterKey=KeyPair,ParameterValue=EffectiveDevOpsAWS</span><br><span class="line">aws cloudformation wait stack-create-complete --stack-name helloworld-production</span><br><span class="line">arn=$(aws deploy get-deployment-group --application-name helloworld --deployment-group-name staging --query &apos;deploymentGroupInfo.serviceRoleArn&apos;)</span><br><span class="line">aws deploy create-deployment-group --application-name helloworld --ec2-tag-filters Key=aws:cloudformation:stack-name,Type=KEY_AND_VALUE,Value=helloworld-production --deployment-group-name production --service-role-arn $arn</span><br><span class="line">aws deploy create-deployment-group --application-name helloworld --ec2-tag-filters Key=aws:cloudformation:stack-name,Type=KEY_AND_VALUE,Value=helloworld-production --deployment-group-name production --service-role-arn arn:aws:iam::012152306932:role/CodeDeployRole</span><br><span class="line">aws deploy list-deployment-groups --application-name helloworld</span><br><span class="line">aws sns create-topic --name production-deploy-approval</span><br><span class="line">aws sns subscribe --topic-arn arn:aws:sns:ap-southeast-2:012152306932:production-deploy-approval --protocol email --notification-endpoint foobar@gmail.com</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/stanosaka/ansible&quot;&gt;source code ansible&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/00root
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>vagrant</title>
    <link href="http://223.95.78.227/2019/08/03/vagrant/"/>
    <id>http://223.95.78.227/2019/08/03/vagrant/</id>
    <published>2019-08-03T13:35:53.000Z</published>
    <updated>2019-08-05T05:20:28.770Z</updated>
    
    <content type="html"><![CDATA[<p>Vagrant can be used for automatically provisioning VMs, and even whole development environments.</p><p>On ubuntu host:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install vagrant</span><br><span class="line">$ vagrant --version</span><br><span class="line">$ mkdir Vagrant;cd Vagrant</span><br><span class="line">$ vagrant init</span><br><span class="line"># Every Vagrant development environment requires a box. You can search for</span><br><span class="line">  # boxes at https://vagrantcloud.com/search.</span><br><span class="line">  config.vm.box = &quot;base&quot;</span><br><span class="line">$ sed -i &apos;s#config.vm.box = &quot;base&quot;#config.vm.box = &quot;centos/7&quot;#g&apos; Vagrantfile</span><br><span class="line">$ vagrant up</span><br><span class="line">$ vagrant ssh</span><br><span class="line">$ vagrant destroy</span><br></pre></td></tr></table></figure></p><p><strong>Components</strong></p><ul><li>Providers:<ul><li>Backend of Vagrant</li><li>VirtualBox</li><li>VMware</li><li>Hyper-V</li><li>vCloud</li><li>AWS</li></ul></li><li>Boxes:<ul><li>Predefined images</li><li><a href="https://app.vagrantup.com/boxes/search">Public Vagrant box catalog</a></li></ul></li><li>Vagrantfile:<ul><li>A Ruby file</li><li>How many VMs</li><li>Configure VMs</li><li>Provision VMs</li><li>Committed to version control<br><img src="https://i.imgur.com/lb7muP9.png" alt="example of Vagrantfile"></li></ul></li><li>Provisioners:<ul><li>Automatically install software, alter configurations</li><li>Boxes may not be a complete use case for you</li><li>Multiple options</li><li>Shell Script</li><li>Ansible</li><li>Chef</li><li>Docker</li><li>Puppet<br><img src="https://i.imgur.com/U5cISEK.png" alt="workflow of vagrant"></li></ul></li></ul><p><strong>Operations</strong></p><ul><li>Adding a vagrant box:<ul><li>Syntax: vagrant box add <name> <url> <provider></li><li>Example: vagrant box add ubuntu/trusty32</li></ul></li><li>Listing and removing vagrant boxes:<ul><li>vagrant box list</li><li>vagrant box remove</li></ul></li><li>Creating a VM environment:<ul><li>Syntax: vagrant init <your box name></li><li>Example: vagrant init ubuntu/trusty32</li></ul></li><li>Starting a VM environment:<ul><li>vagrant up ubuntu/trusty32</li><li>vagrant up </li></ul></li><li>Connecting:<ul><li>vagrant ssh ubuntu/trusty32</li><li>vagrant ssh</li></ul></li><li>Stopping, restarting, and destroying<ul><li>vagrant halt</li><li>vagrant reload</li><li>vagrant destroy</li></ul></li></ul><p><strong>Provisioning of vagrant</strong></p><ul><li>Creating a VM and provisioning it with Apache installed and prot forwarding</li><li><p>Add theses lines in Vagrantfile:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080</span><br><span class="line">config.vm.provision &apos;shell&apos;, path: &apos;provision.sh&apos;</span><br></pre></td></tr></table></figure></li><li><p>Create provision.sh file with these entries:</p><ul><li>sudo apt-get update</li><li>sudo apt-get install -y apache2</li></ul></li><li><p>Destroy and start the virtual machine:</p><ul><li>vagrant destroy</li><li>vagrant up</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Vagrant can be used for automatically provisioning VMs, and even whole development environments.&lt;/p&gt;
&lt;p&gt;On ubuntu host:&lt;br&gt;&lt;figure class=
      
    
    </summary>
    
    
      <category term="vagrant" scheme="http://223.95.78.227/tags/vagrant/"/>
    
      <category term="Hashicorp" scheme="http://223.95.78.227/tags/Hashicorp/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins Pipeline</title>
    <link href="http://223.95.78.227/2019/07/29/jenkins-blue-ocean/"/>
    <id>http://223.95.78.227/2019/07/29/jenkins-blue-ocean/</id>
    <published>2019-07-29T01:00:54.000Z</published>
    <updated>2019-09-17T10:46:54.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-Example-of-Declarative-Pipeline"><a href="#An-Example-of-Declarative-Pipeline" class="headerlink" title="An Example of Declarative Pipeline:"></a>An Example of Declarative Pipeline:</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent &#123;</span><br><span class="line">       node &#123;</span><br><span class="line">           label <span class="string">'master'</span></span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">       stage(<span class="string">'Build'</span>) &#123;</span><br><span class="line">           steps &#123;</span><br><span class="line">               sh <span class="string">'mvn clean verify -DskipITs=true'</span></span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       stage (<span class="string">'Static Code Analysis'</span>) &#123;</span><br><span class="line">           steps &#123;</span><br><span class="line">               sh <span class="string">'mvn clean verify sonar:sonar'</span></span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       stage (<span class="string">'Publish to Artifactory'</span>) &#123;</span><br><span class="line">           steps &#123;</span><br><span class="line">               script &#123;</span><br><span class="line">                   def server = Artifactory.server <span class="string">'Default Artifactory'</span></span><br><span class="line">                   def uploadSpec = <span class="string">""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">                       "</span>files<span class="string">": [</span></span><br><span class="line"><span class="string">                           &#123;</span></span><br><span class="line"><span class="string">                               "</span>pattern<span class="string">": "</span>target/*.war<span class="string">",</span></span><br><span class="line"><span class="string">                               "</span>target<span class="string">": "</span>helloworld/<span class="variable">$&#123;BUILD_NUMBER&#125;</span>/<span class="string">",</span></span><br><span class="line"><span class="string">                               "</span>pattern<span class="string">": "</span>Unit-Tested=Yes<span class="string">",</span></span><br><span class="line"><span class="string">                           &#125;</span></span><br><span class="line"><span class="string">                        ]</span></span><br><span class="line"><span class="string">                    &#125;"</span><span class="string">""</span></span><br><span class="line">                    server.upload(uploadSpec)</span><br><span class="line">&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Jenkinsfile</strong><br>A Jenkinsfile is a file that contains Pipeline code.</p><h1 id="Shared-Library"><a href="#Shared-Library" class="headerlink" title="Shared Library"></a>Shared Library</h1><p>A Jenkins Shared Library is an external source control repository containing your complex Groovy code. It acts like a function that could be used on-demand inside your Declarative Pipeline.</p><p><em>A Groovy Script (example.groovy)Inside Jenkins Shared Library Repository:</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def greet(message) &#123;</span><br><span class="line">   echo &quot;Hello $&#123;message&#125;, welcome to Jenkins Blue Ocean.&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>Jenkins Declarative Pipeline Utilizing the Shared Library:</em><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Library(&apos;Example_shared_Library&apos;) _</span><br><span class="line"></span><br><span class="line">pipeline &#123;</span><br><span class="line">  agent none</span><br><span class="line">  stage (&apos;Example&apos;) &#123;</span><br><span class="line">     steps &#123;</span><br><span class="line">        script &#123;</span><br><span class="line">           example.greet &apos;Readers&apos;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Setting-up-Jenkins-Blue-Ocean"><a href="#Setting-up-Jenkins-Blue-Ocean" class="headerlink" title="Setting up Jenkins Blue Ocean"></a>Setting up Jenkins Blue Ocean</h1><h2 id="Setting-up-Blue-Ocean-using-docker"><a href="#Setting-up-Blue-Ocean-using-docker" class="headerlink" title="Setting up Blue Ocean using docker"></a>Setting up Blue Ocean using docker</h2><p><strong>Downloading the latest Jenkins Blue Ocean</strong></p><ol><li><p>docker pull jenkinsci/blueocean</p></li><li><p>To list the downloaded docker image: docker images</p></li></ol><p>Docker containers generate and use data. When a container gets deleted, its relevant data also gets lost.<br>To make the data persistent, we use docker volumes.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker volume create jenkins_home</span><br><span class="line">docker volume ls</span><br><span class="line">docker volume inspect jenkins_home <span class="comment">#get detailed info about docker volume</span></span><br></pre></td></tr></table></figure></p><h2 id="Run-Jenkins-blue-ocrean-behind-a-reverse-proxy"><a href="#Run-Jenkins-blue-ocrean-behind-a-reverse-proxy" class="headerlink" title="Run Jenkins blue ocrean behind a reverse proxy"></a>Run Jenkins blue ocrean behind a reverse proxy</h2><ol><li><p>Run a Jenkins container</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name jenkins -v jenkins_home:/var/jenkins_home jenkinsci/blueocean</span><br></pre></td></tr></table></figure></li><li><p>Download the docker image for Nginx</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nginx</span><br></pre></td></tr></table></figure></li><li><p>Spawn a Dockr container for Nginx. Also, link the nginx container to the jenkins container using the <em>–link</em> option.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name ngingx -p 80:80 --link jenkins nginx</span><br></pre></td></tr></table></figure></li><li><p>Get inside the Ningx container using the <em>docker exec</em> command</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it ningx /bin/bash</span><br></pre></td></tr></table></figure></li><li><p>Update the Ubuntu package lists<br><code>apt-get update</code></p></li><li>Install vim text editor<br><code>apt-get install vim</code></li><li><p>Take the backup of the default.conf file inside /etc/nginx/conf.d/</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp etc/ningx/conf.d/default.conf etc/nginx/conf.d/default.conf.backup</span><br></pre></td></tr></table></figure></li><li><p>Next, replace the content of the default.conf file with the following:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">upstream jenkins &#123;</span><br><span class="line">  server jenkins:8080;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">  listen 80;</span><br><span class="line">  server_name jenkins.example.com;</span><br><span class="line">  </span><br><span class="line">  location / &#123;</span><br><span class="line">     proxy_pass         http://jenkins;</span><br><span class="line">     proxy_set_header   Host <span class="variable">$host</span>;</span><br><span class="line">     proxy_set_header   X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">     proxy_set_header   X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">     proxy_set_header   X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Exit the Nginx container.<br><code>exit</code></p></li><li>Restart the Nginx container.<br><code>docker restart nginx</code></li><li>Run the following docker command to fetch the content of initialAdminPassword file.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it jenkins /bin/bash -c \ <span class="string">"cat /var/jenkins_home/secrets/initialAdminPassword"</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="Creating-Pipeline"><a href="#Creating-Pipeline" class="headerlink" title="Creating Pipeline"></a>Creating Pipeline</h1><p><strong>Prerequisites</strong>     </p><ol><li>fork <a href="https://github.com/Apress/beginning-jenkins-blue-ocean/tree/master/Ch03/example-maven-project">example maven project</a></li><li>pulling the docker image for jenkins agent<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nikhilpathania/jenkins_ssh_agent</span><br></pre></td></tr></table></figure></li></ol><p>The docker image is based on Ubuntu and comes with Git,Java JDK, Maven, and sshd installed.<br>The image also contains a user account named <em>jenkins</em>.</p><p><strong>Creating credentials for the docker image in Jenkins</strong><br>Add credentials inside Jenkins that allow it to interact with the docker image nikhilpathania/jenkins_ssh_agent</p><ol><li>Classic Jenkins dashboard -&gt; Credentials-&gt; System-&gt; Global credential (unrestricted).</li><li>Add Credentials</li><li>Options<ol><li>Kind: Username with password</li><li>Username: the username to interact with the docker image: jenkins</li><li>Password: the password to interact with the docker image: jenkins</li><li>ID: add a meaningful name to recognize these credentials.</li><li>Descritpion: Add a meaningful description for these credentials.<br><img src="https://i.imgur.com/pFaHQuZ.png" alt="configuring the credential"><br><strong>Installing the docker plugin</strong><br>To spawn on-demand docker containers serving as Jenkins agents, need to install the docker plugin<br><img src="https://i.imgur.com/nUi1uKk.png" alt="Installing the Docker Plugin for Jenkins">   </li></ol></li></ol><p><strong>Configuring the docker plugin</strong><br>Manage Jenkins-&gt; Configure System-&gt; Cloud-&gt; Add a new cloud-&gt; Docker</p><p>Options to configure:</p><ol><li>Docker Host URI: This is the URL used by Jenkins to talk to the docker host.</li></ol><hr><p><strong>ENABLING DOCKER REMOTE API (CRITICAL)</strong><br>The Docker remote API allows external applications to communicate with the Docker server using REST APIs . Jenkins (through the Docker Plugin) uses the docker remote API to communicate with a docker host.</p><p>To enable the Docker remote API on your Docker host, you’ll need to modify Docker’s configuration file. Depending on your OS version and the way you have installed Docker on your machine, you might need to choose the right configuration file to modify. Shown here are two methods that work on Ubuntu. Try them one by one.</p><p><strong>Modifying the docker.conf File</strong><br>Follow these steps to modify the docker.conf file :</p><ol><li>Log in to your docker server; make sure you have sudo privileges.</li></ol><ol start="2"><li>Execute the following command to edit the file docker.conf :</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/init/docker.conf</span><br></pre></td></tr></table></figure><ol start="3"><li>Inside the docker.conf file, go to the line containing “DOCKER_OPTS=” .</li></ol><p>You’ll find “DOCKER_OPTS=” variable at multiple places inside the docker.conf file. Use the DOCKER_OPTS= that is available under the pre-start script or script section.</p><ol start="4"><li>Set the value of DOCKER_OPTS as shown here. Do not copy and paste; type it all in a single line.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOCKER_OPTS=&apos;-H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock&apos;</span><br></pre></td></tr></table></figure></li></ol><p>The above setting binds the Docker server to the UNIX socket as well on TCP port 4243.</p><p>“0.0.0.0” makes Docker engine accept connections from anywhere. If you would like your Docker server to accept connections only from your Jenkins server, then replace “0.0.0.0” with your Jenkins Server IP.</p><ol start="5"><li>Restart the Docker server using the following command:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></li></ol><ol start="6"><li>To check if the configuration has worked, execute the following command. It lists all the images currently present on your Docker server.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X GET http://&lt;Docker Server IP&gt;:4243/images/json</span><br></pre></td></tr></table></figure></li></ol><ol start="7"><li>If this command does not return a meaningful output, try the next method.</li></ol><p><strong>Modifying the docker.service File</strong><br>Follow these steps to modify the docker.service file :</p><ol><li>Execute the following command to edit the docker.service file.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li>Inside the docker.service file, go to the line containing ExecStart= and set its value as shown here. Do not copy and paste; type it all in a single line.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:4243</span><br></pre></td></tr></table></figure></li></ol><p>The above setting binds the docker server to the UNIX socket as well on TCP port 4243.</p><p>“0.0.0.0” makes the Docker engine accept connections from anywhere. If you would like your Docker server to accept connections only from your Jenkins server, then replace “0.0.0.0” with your Jenkins Server IP.</p><ol start="3"><li>Execute the following command to make the Docker demon notice the modified configuration:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li>Restart the Docker server using the below command:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service docker restart</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li>To check if the configuration has worked, execute the following command. It lists all the images currently present on your Docker server.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X GET http://&lt;Docker Server IP&gt;:4243/images/json</span><br></pre></td></tr></table></figure></li></ol><hr><ol start="2"><li>Server Credentials: If your docker host requireds a login, you need to add the credentials to Jenkins using the <strong>Add</strong> button. However, do nothing if you are using a docker host that’s running your Jenkins server container.</li><li>Test Connection: Click on this to test the communication between your Jenkins server and the docker host. You should see the docker version and the API version [4] if the connection is successful.</li><li>Enabled: A checkbox to enable/disable the current configuration.<br><img src="https://i.imgur.com/RKOm2V2.png" alt="Configuring the Docker host URI and testing the connection"></li></ol><p><strong>Add Docker Template</strong> button. Click on it to configure the docker image that Jenkins shoudl use to spawn container.<br><img src="https://i.imgur.com/6NOb5kY.png" alt="docker template figure"></p><ol><li>Labels: The label that you type in under the Labels field gets used inside your Pipeline to define agents for your stages. In this way, Jenkins knows that it has to use docker to spawn agents. </li><li>Enabled: This checkbox is used to enable/disable the current configuration. </li><li>Docker Image: Add the name of the docker image that should be used to spawn agents containers. </li><li>Remote File System Root: This is the directory inside the container that holds the workspace of the Pipeline that runs inside it. </li><li>Usage: We would like only to build pipelines that have the right agent label, in our case it is docker.</li><li>Connect method : Choose to Connect with SSH option to allow Jenkins to connect with the container using the SSH protocol.</li><li>SSH Key: Choose use configured SSH credentials from the options to use the SSH credentials as the preferred mode of authentication.</li><li>SSH Credentials: From the list of options choose the credentials that you have created earlier, in the section: Creating Credentials for the Docker Image in Jenkins.</li><li>Host Key Verification Strategy: Choose Non verifying Verification Strategy to keep things simple. However, this is not the recommended setting for a production Jenkins server.<h1 id="Using-the-pipeline-creation-wizard-configure-Jenkins-Blue-Ocean-with-various-types-of-source-code-repositories"><a href="#Using-the-pipeline-creation-wizard-configure-Jenkins-Blue-Ocean-with-various-types-of-source-code-repositories" class="headerlink" title="Using the pipeline creation wizard, configure Jenkins Blue Ocean with various types of source code repositories."></a>Using the pipeline creation wizard, configure Jenkins Blue Ocean with various types of source code repositories.</h1><h1 id="Using-the-Visual-Pipeline-Editor-to-design-Pipeline"><a href="#Using-the-Visual-Pipeline-Editor-to-design-Pipeline" class="headerlink" title="Using the Visual Pipeline Editor to design Pipeline."></a>Using the Visual Pipeline Editor to design Pipeline.</h1></li></ol><ul><li>Downloads the source code from the Github repository</li><li>Performs a build and some testing</li><li>Publishes the testing results under the Pipeline’s Test page</li><li>Uploads the built artifact to Henkins Blue Ocean</li></ul><p><strong>Assigning a global agent</strong><br>The pipeline that you are going to create should have two stages, and each stage is supposed to run inside a docker<br>container. You’ll define the agents for each stage sparately in the stage’s settings.<br>Therefore, let’s keep the Pipeline’s global agent setting to none.<br><img src="https://i.imgur.com/fBKsY4k.png" alt="Assigning a global agent"></p><p><strong>Creating a build &amp; test stage</strong><br>Type in the name <strong>Build &amp; Test</strong> for your stage.<br><img src="https://i.imgur.com/SOXKlCZ.png" alt="Naming your stage"></p><p><strong>Adding steps</strong><br>Let’s add some steps to our <strong>Build &amp; Test</strong> stage.<br><img src="https://i.imgur.com/FR5koOy.png" alt="Adding a new step"><br><strong>Adding a shell script setp</strong><br>Our source code is a Maven project, and we would like to build and test it using an mvn command, which eventually<br>gets executed inside a shell on the Jenkins agent.<br><img src="https://i.imgur.com/zdRO91W.png" alt="Adding a shell script step"><br>Paste the below code which is a maven command to build, test, and create a package out of your source code.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn -Dmaven.test,failure,ignore clean package</span><br></pre></td></tr></table></figure></p><p><strong>Adding a stash step to pass artifact between stages</strong><br>Add another step to stash the build package and the testing report generated by the maven command.<br>Look for the step <strong>Stash some files to be used later in the build</strong><br><img src="https://i.imgur.com/yguB0hB.png" alt="Adding a Stash step"></p><p>Add the name “build-test-artifacts” fro your stash using the Name<em> field, which is mandatory.<br>Add the following to the Included field: **/target/surefire-reports/TEST-</em>.xml,target/*.jar</p><p>With this configuration you are telling Jenkins to stash any .jar file (build package) from the target directory,<br>and the TEST-*.xml file(test report) from the <code>**/target/surefire-reports/</code> directory on the build agent.<br><img src="https://i.imgur.com/ZtwyY00.png" alt="configuring a Stash step"></p><p>piple line code so far:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent none</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&apos;Build &amp; Test&apos;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &apos;mvn -Dmaven.test.failure.ignore clean package&apos;</span><br><span class="line">                stash(name: &apos;build-test-artifacts&apos;, \</span><br><span class="line">                includes: &apos;**/target/surefire-reports/TEST-*.xml,target/*.jar&apos;)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>Assigning an agent for the build &amp; test stage</strong><br>You’ll assign a build agent for your <strong>Build &amp; Test</strong> stage. The agent is going to be a docker container that will be spawn automatically by Jenkins. ONce the stage is complete, Jenkins will destroy the container.<br><img src="https://i.imgur.com/8QGv9eH.png" alt="assigning an agent to a stage"><br>With the following configuration, Jenkins looks for an agent with the label <strong>docker</strong>. Remember the section, wherein you configured the docker plugin in Jenkins. You specified the label <strong>docker</strong> while configure the <strong>Docker Agent<br>Template</strong>.</p><p><strong>Creating a report &amp; publish stage</strong><br>Add another stage named <strong>Report &amp; Publish</strong> that will publish the testing results on the <strong>Test</strong> page of the Pipeline and that will publish the build package on the <strong>Artifacts</strong> page of the pipeline.<br><img src="https://i.imgur.com/1wyyhNT.png" alt="Naming your stage"></p><p><strong>Adding an un-stash step</strong><br>Before we do anything in the <strong>Report &amp; Publish</strong> stage. it is first crucial to un-stash the files that were stashed<br>in the previous stage. So let’s add step to un-stash a stash from the previous stage.<br><img src="https://i.imgur.com/e0XDMmW.png" alt="Adding a restore files previously stashed step"><br>You’ll see a text field Name* where you should paste the name of your stash precisely as it was defined during its<br>creation.<br><img src="https://i.imgur.com/O5Do1D7.png" alt="Configuring the Restore files previously stashed step"></p><p><strong>Report testing results</strong><br>The stash contains a JUnit test results .xml file that you’ll publish on the pipeline’s Test page. For this, we need<br>to add a step named <strong>Archive Junit-formatted test results</strong></p><p>Use the TestResults<em> field to provide Jenkins with the path to your JUnit test result file. In our case, it is<br>`**/target/surefire-reports/TEST-</em>.xml`</p><p><img src="https://i.imgur.com/Skz6Yg7.png" alt="Configuring an Archive Junit-formatted test results step"></p><p><strong>Upload artifacts to blue ocean</strong><br>Add a step that will upload the build package to the Pipleine’s Artifacts page. From the un-stashed files, you also<br>have a .jar file that is the build package.</p><p>To upload it to the Pipeline Artifacts page, use the <strong>Archive the artifacts</strong> step.<br>Use the Artifacts<em> filed to provide Jenkins with the path to your build package file. In our case, it is target/</em>.jar.Also, tick the option OnlyIfSuccessful to upload the artifacts only if the Pipeline status is green or yellow.<br><img src="https://i.imgur.com/X1ogakY.png" alt="Configuring the Archive the Artifacts step"> </p><p><strong>Assigning an aget for the report &amp; publish stage</strong><br>you’ll assign a build agent for your Report &amp; Publish stage. The agent is going to be a docker container that will be spawn automatically by Jenkins. Once the stage is complete, Jenkins will destroy the container.</p><p><img src="https://i.imgur.com/zg4Jc5G.png" alt="Assigning an agent to a stage"></p><p>You are new done with creating the pipeline. To save the changes, click on the Save button.</p><p>When you click on the <strong>Save</strong> button, Jenkins in the back end converts your UI configurations into a Jenkinsfile that follows the Declarative Pipeline Syntax.</p><p><img src="https://i.imgur.com/2HYkBP5.png" alt="Committing your pipeline configurations changes"></p><p><strong>Run an artifactory server</strong><br>To spawn an Artifactory server with docker. Artifactory is a popular tool to manage and version control software build artifacts. </p><ol><li>Log in to your docker host</li><li>docker volume create –name artifactory_data</li><li>docker pull docker.bintray.io/jfrog/artifactory-oss:latest  # downloading the latest version of Artifactory community edition</li><li>docker run –name artifactory -d -v artifactory_data:/var/opt/jfrog/ -p 8081:8081 docker.bintray.io/jfrog/artifactory-oss:latest</li><li>access your Artifactory server: http://&lt;Docker_host_ip&gt;:8081/artifactory/webapp/#/home</li><li>using the admin credentials (username: admin, and password as password). Note the default example repository in Artifactory named example-repo-local.</li></ol><p><strong>Installing the artifactory plugin for jenkins</strong><br>Manage Jenkins-&gt; Manage Plugins-&gt; Artifactory</p><p><strong>Configuring the artifactory plugin in jenkins</strong><br><img src="https://i.imgur.com/2HYkBP5.png" alt="configuring the artifactory plugin"></p><p><strong>Creating a Publish to Artifactory Stage (Parallel Stage)</strong><br>Add a stage in parallel to existing <strong>Report &amp; Publish</strong> stage.<br><img src="https://i.imgur.com/HAq6sYq.png" alt="creating a new stage"><br><img src="https://i.imgur.com/vXUpngZ.png" alt="naming your stage"><br>Your new stage first downloads the stash files from the previous stage, and then it publishes the built artifacts to<br>the Artifactory server.</p><p><strong>Adding a scripted pipeline step</strong><br>does two things</p><ol><li>it fetches the stash files from the previous stage.</li><li>it runs a filespec that uploads the build package to the Artifactory server.<br><img src="https://i.imgur.com/9W9fTUQ.png" alt="add a scripted pipeline step"><br>Script to Un-Stash Built Artifacts and Upload Them to the Artifactory Server<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">unstash &apos;build-test-artifacts&apos;</span><br><span class="line">def server = Artifactory.server &apos;Artifactory&apos;</span><br><span class="line">def uploadSpec = &quot;&quot;&quot;&#123;</span><br><span class="line">  &quot;files&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;pattern&quot;: &quot;target/*.jar&quot;,</span><br><span class="line">      &quot;target&quot;: &quot;example-repo-local/$&#123;BRANCH_NAME&#125;/$&#123;BUILD_NUMBER&#125;/&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;&quot;&quot;&quot;</span><br><span class="line">server.upload(uploadSpec)</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">███████╗██╗  ██╗██████╗ ██╗      █████╗ ██╗███╗   ██╗</span><br><span class="line">██╔════╝╚██╗██╔╝██╔══██╗██║     ██╔══██╗██║████╗  ██║</span><br><span class="line">█████╗   ╚███╔╝ ██████╔╝██║     ███████║██║██╔██╗ ██║</span><br><span class="line">██╔══╝   ██╔██╗ ██╔═══╝ ██║     ██╔══██║██║██║╚██╗██║</span><br><span class="line">███████╗██╔╝ ██╗██║     ███████╗██║  ██║██║██║ ╚████║</span><br><span class="line">╚══════╝╚═╝  ╚═╝╚═╝     ╚══════╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝</span><br></pre></td></tr></table></figure><p>The code line unstash ‘build-test-artifacts’ downloads the previously stashed package. The rest of the code is a Filespec that uploads the target/*jar file, which is our built package file, to the Artifactory server on the repository example-repo-local.</p><p>Notice that the target path contains Jenkins Global variables, ${BRANCH_NAME} and ${BUILD_NUMBER}, representing the branch name and build number, respectively. Doing so uploads the built artifacts to a unique path on Artifactory every single time a pipeline runs.</p><p><strong>Assigning an Agent for the Publish to Artifactory Stage</strong><br>you’ll assign a build agent for your Publish to Artifactory stage. The agent is going to be the docker container that gets spawned automatically by Jenkins. Once the stage is complete, Jenkins destroys the container.<br><img src="https://i.imgur.com/l3VMpHu.png" alt="Assigning an agent to a stage"></p><p>Final pipeline code:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">  agent none</span><br><span class="line">  stages &#123;</span><br><span class="line">    stage(&apos;Build &amp; Test&apos;) &#123;</span><br><span class="line">      agent &#123;</span><br><span class="line">        node &#123;</span><br><span class="line">          label &apos;docker&apos;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">      steps &#123;</span><br><span class="line">        sh &apos;mvn -Dmaven.test.failure.ignore clean package&apos;</span><br><span class="line">        stash(name: &apos;build-test-artifacts&apos;, includes: &apos;**/target/surefire-reports/TEST-*.xml,target/*.jar&apos;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(&apos;Report &amp; Publish&apos;) &#123;</span><br><span class="line">      parallel &#123;</span><br><span class="line">        stage(&apos;Report &amp; Publish&apos;) &#123;</span><br><span class="line">          agent &#123;</span><br><span class="line">            node &#123;</span><br><span class="line">              label &apos;docker&apos;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">          &#125;</span><br><span class="line">          steps &#123;</span><br><span class="line">            unstash &apos;build-test-artifacts&apos;</span><br><span class="line">            junit &apos;**/target/surefire-reports/TEST-*.xml&apos;</span><br><span class="line">            archiveArtifacts(artifacts: &apos;target/*.jar&apos;, onlyIfSuccessful: true)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Publish to Artifactory&apos;) &#123;</span><br><span class="line">          agent &#123;</span><br><span class="line">            node &#123;</span><br><span class="line">              label &apos;docker&apos;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">          &#125;</span><br><span class="line">          steps &#123;</span><br><span class="line">            script &#123;</span><br><span class="line">              unstash &apos;build-test-artifacts&apos;</span><br><span class="line"></span><br><span class="line">              def server = Artifactory.server &apos;Artifactory&apos;</span><br><span class="line">              def uploadSpec = &quot;&quot;&quot;&#123;</span><br><span class="line">                &quot;files&quot;: [</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;pattern&quot;: &quot;target/*.jar&quot;,</span><br><span class="line">                    &quot;target&quot;: &quot;example-repo-local/$&#123;BRANCH_NAME&#125;/$&#123;BUILD_NUMBER&#125;/&quot;</span><br><span class="line">                  &#125;</span><br><span class="line">                ]</span><br><span class="line">              &#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">              server.upload(uploadSpec)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><img src="https://i.imgur.com/id9vL50.png" alt="Committing your pipeline configurations changes"><br><img src="https://i.imgur.com/Ec4LinF.png" alt="build artifact uploaded to Artifactory"></p><p><strong>Running a pipeline for a pull requrest</strong><br>Jenkins Blue Ocean can detect pull requests on your Github repositories and run a pipeline with it for you. The<br>pipeline run result (fail/pass/canceled) gets reported back to your source code repository.</p><p>The person who is reponsible for accepting the pull request can then decide based on the pipeline run result whether<br>he should merge the new changes into the destination branch or not.<br><img src="https://i.imgur.com/kUvGGZM.jpg" alt="pull request"></p><h2 id="Issue"><a href="#Issue" class="headerlink" title="Issue"></a>Issue</h2><p>pull requests not showing<br>fix by: <img src="https://i.imgur.com/63ETSbk.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;An-Example-of-Declarative-Pipeline&quot;&gt;&lt;a href=&quot;#An-Example-of-Declarative-Pipeline&quot; class=&quot;headerlink&quot; title=&quot;An Example of Declarativ
      
    
    </summary>
    
    
      <category term="Jenkins" scheme="http://223.95.78.227/tags/Jenkins/"/>
    
      <category term="Devops" scheme="http://223.95.78.227/tags/Devops/"/>
    
      <category term="CD/CI" scheme="http://223.95.78.227/tags/CD-CI/"/>
    
      <category term="Pipeline" scheme="http://223.95.78.227/tags/Pipeline/"/>
    
      <category term="Blue Ocean" scheme="http://223.95.78.227/tags/Blue-Ocean/"/>
    
  </entry>
  
  <entry>
    <title>LinuxSystemAdmin</title>
    <link href="http://223.95.78.227/2019/07/23/LinuxSystemAdmin/"/>
    <id>http://223.95.78.227/2019/07/23/LinuxSystemAdmin/</id>
    <published>2019-07-22T23:00:22.000Z</published>
    <updated>2020-03-23T05:13:40.268Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TCP-IP-networking"><a href="#TCP-IP-networking" class="headerlink" title="TCP/IP networking"></a>TCP/IP networking</h1><h2 id="Troubleshoot-networking"><a href="#Troubleshoot-networking" class="headerlink" title="Troubleshoot networking"></a>Troubleshoot networking</h2><p><strong>Tools for troubleshooting the network</strong></p><ul><li><em>ping</em> - ICMP echo requests<br><img src="https://i.imgur.com/RcrCjea.jpg" alt="ping -I eth1 192.168.10.12"></li><li><em>traceroute</em> and <em>tracepath</em> - Trace the path taken to a given host</li><li><em>netcat</em> - Arbitrary TCP and UDP network communication<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">stan@stan-virtual-machine:~$ ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 00:0c:29:a2:c2:5d  </span><br><span class="line">          inet addr:192.168.199.107  Bcast:192.168.199.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::20c:29ff:fea2:c25d/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:109798 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:34545 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:53893408 (53.8 MB)  TX bytes:15357635 (15.3 MB)</span><br><span class="line"></span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:5917 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:5917 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:1699426 (1.6 MB)  TX bytes:1699426 (1.6 MB)</span><br><span class="line"></span><br><span class="line">stan@stan-virtual-machine:~$ nc -l 8000</span><br><span class="line">nc -v -z 192.168.199.107 8000</span><br><span class="line">Connection to 192.168.199.107 8000 port [tcp/*] succeeded!</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://i.imgur.com/60o3bav.png" alt="nc"></p><ul><li><em>tcpdump</em> and <em>wireshark</em> - Packet captures for network analysis<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> nc -l 8000 &amp;</span><br><span class="line">[1] 20393</span><br><span class="line">➜  ~ sudo tcpdump -i enp2s0 port 8000</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on enp2s0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">Hi</span><br><span class="line">[1]  + 20393 done       nc -l 8000</span><br><span class="line">09:52:41.674092 IP stan-virtual-machine.lan.57089 &gt; stan-OptiPlex-380.lan.8000: Flags [S], seq 3581133462, win 29200, options [mss 1460,sackOK,TS val 12349350 ecr 0,nop,wscale 7], length 0</span><br><span class="line">09:52:41.674180 IP stan-OptiPlex-380.lan.8000 &gt; stan-virtual-machine.lan.57089: Flags [S.], seq 2419171515, ack 3581133463, win 28960, options [mss 1460,sackOK,TS val 2113949133 ecr 12349350,nop,wscale 7], length 0</span><br><span class="line">09:52:41.674412 IP stan-virtual-machine.lan.57089 &gt; stan-OptiPlex-380.lan.8000: Flags [.], ack 1, win 229, options [nop,nop,TS val 12349351 ecr 2113949133], length 0</span><br><span class="line">09:52:41.674518 IP stan-virtual-machine.lan.57089 &gt; stan-OptiPlex-380.lan.8000: Flags [P.], seq 1:4, ack 1, win 229, options [nop,nop,TS val 12349351 ecr 2113949133], length 3</span><br><span class="line">09:52:41.674541 IP stan-OptiPlex-380.lan.8000 &gt; stan-virtual-machine.lan.57089: Flags [.], ack 4, win 227, options [nop,nop,TS val 2113949133 ecr 12349351], length 0</span><br><span class="line">09:52:41.674582 IP stan-virtual-machine.lan.57089 &gt; stan-OptiPlex-380.lan.8000: Flags [F.], seq 4, ack 1, win 229, options [nop,nop,TS val 12349351 ecr 2113949133], length 0</span><br><span class="line">09:52:41.674718 IP stan-OptiPlex-380.lan.8000 &gt; stan-virtual-machine.lan.57089: Flags [F.], seq 1, ack 5, win 227, options [nop,nop,TS val 2113949133 ecr 12349351], length 0</span><br><span class="line">^C</span><br><span class="line">7 packets captured</span><br><span class="line">8 packets received by filter</span><br><span class="line">1 packet dropped by kernel</span><br><span class="line"></span><br><span class="line">stan@stan-virtual-machine:~$ echo Hi| nc 192.168.199.178 8000</span><br><span class="line">tcpdump -i eth1 port 8000 and not port 22 and not icmp</span><br><span class="line">tcpdump -i eth1 not udp 53</span><br><span class="line">tcpdump -nX -i eth1 port 8000</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://i.imgur.com/D7Vewu7.jpg" alt="tcpdump"><br><img src="https://i.imgur.com/4WIQHtZ.png" alt="tcp three-way handshake"></p><h2 id="Backup-and-streaming"><a href="#Backup-and-streaming" class="headerlink" title="Backup and streaming"></a>Backup and streaming</h2><p><strong>What to expect from a backup tool?</strong></p><ul><li>Any backup solution should -roughly- provide the following:</li><li>Full and incrementail backups</li><li>File permissions and ownership preservation</li><li>The ability to be automated</li></ul><p><strong>Introducing rsync</strong></p><ul><li>Rsync is native Linux tool that can be deployed from the official repositories</li><li>It supports incremental and full backups- It transfers files over SSH</li><li>It can be automated via cron jobs to run in unattended mode.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">type rsync</span><br><span class="line">rsync -zvr simple-php-website/ ~/backup/</span><br><span class="line">sudo rsync -azv simple-php-website/ ~/backup/ # backup with file created time stampe and ownership</span><br></pre></td></tr></table></figure></li></ul><p><strong>using rsync over the network</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rsync -azv simple-php-website/ pi@rpi-01:~/backup/</span><br><span class="line">rsync -azv  pi@rpi-01:~/backup/ simple-php-website</span><br></pre></td></tr></table></figure></p><p><strong>advanced ssh options with rsync</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id pi@rpi-01 </span><br><span class="line">rsync -avz -e &quot;ssh -p 2222&quot; simple-php-website/ pi@rpi-01:~/backup/ # specify the ssh port</span><br><span class="line">rsync -azv --existing simple-php-website/ pi@rpi-01:~/backup/ ## only sync the file existing in destnation</span><br><span class="line">rsync -avzi simple-php-website/ pi@rpi-01:~/backup/ # i show what has been changed in source and destnation</span><br><span class="line">dd if=/dev/zero of=data.bin bs=102400 count=10240</span><br><span class="line">rsync -azv --progress simple-php-website/ pi@rpi-01:~/backup/ #show the transfers info</span><br><span class="line">rsync -azv --include &apos;*.php&apos; --exclude &apos;*.jpg&apos; simple-php-website/ pi@rpi-01:~/backup/</span><br><span class="line"></span><br><span class="line">```  </span><br><span class="line">## Performance Analysis</span><br><span class="line">**How to improve performance?**</span><br><span class="line">- The following are general guidelines for achieving a higher performance level on a typical Linux box:</span><br><span class="line">  - Make sure that you have enough memory to serve the running applications</span><br><span class="line">  - Use softwre or hardware load balancing systems. They not only provide faster responses from network applications, but they also provide redundancy should one of the servers go down undexpectedly.</span><br><span class="line">  - Review the application specific documentation and configuration files. Some settings may dramatically boost application performance like turning on caching in webservers or unning multiple instances of a network application.</span><br><span class="line">  - Avoid storage I/O bottlnecks by installing faster disks like SSD&apos;s, which do not depend on mechanically moving parts to offer much higher read/write speed than their old counterparts.</span><br><span class="line">  - Use technologies like RAID to distribute I/O evenly on disks (like striping). However, not all applications/databases benefit from striping and RAID and sometimes this my lead to negative results. Application and database vendor and/or documentation should be consulted before moving to RAID.</span><br><span class="line">  - Keep an eye on the network bandwidth and errors to ensure that the bandwidth is not saturated and that the error rate is at the minimum</span><br><span class="line">**Possible causes of bottlenecks**</span><br><span class="line">- Hardware-wise, performance is affected mainly by one or more of the following system components: CPU, memory, and disk and network I/O.</span><br><span class="line">- Processes running on the system must access the above components. They compete to have, for example a CPU cycle or an I/O from the disk to read to write data. If the component is busy, the process will have to wait for its trun to be served. This wait time means that the sytem will run slower and implies that are have a performance issue.</span><br><span class="line">**Check your resources**</span><br><span class="line">- Before addressing a performance degradation problem, you must first check your assets to have an estimate of the upper bound for system&apos;s general performance level</span><br><span class="line">- The following files provide hardware information:</span><br><span class="line">  - /proc/cpuinfo: take note of the vendor ID, cpu family, model and model name. Each processor core will have a stanza of its own. Useful information can be extracted from the CPU flags like ht which means that the CPU is using the hyper threading technology.</span><br><span class="line">  - /proc/meminfo: details on total, used, and free memeory</span><br><span class="line">  - /proc/diskstats: disk devices statistics</span><br><span class="line">- Another useful command for this purpose if dmidecode. This will print a lot of hardware information about the machine like the mothermoard type, BIOS version, installed memory amont many other information.</span><br><span class="line">**Using vmstat to measure CPU utilization**</span><br><span class="line">- When meansuring CPU performance, you may want to determine the overall CPU utilization to know whether or not the overall clock speed is the problem, load averages may also aid you in this. In addition, you may want to check perprocess CPU consumption to know which process really hogging the CPU</span><br><span class="line">- Running vmstat gives you the information you need. It takes the number of seconds and the number of reports as the first and second arguments to determine the number of seconds for which the tool will calculate the averages. The first line of output represents the averages since the systems boot time. The subsequent line present the average per n seconds.</span><br><span class="line">- The right most column is for CPU readings. Us, sy,id, and wa represent the user, system, idle time, and wait time for CPU.</span><br><span class="line">- A high us means that the system is busy doing computational tasks, while a high **sy** time means the system is making a lot of system calls and/or making a lot of I/O requests. A system-typically-should be using no more than 50% in user time, no more than 50% in system time, and have a non-zero idle time.</span><br><span class="line">- The **cs** is short for context switches per interval. That is how many times the kernel switched the running process per interval. The **in** is short for interrupts, it shows the number of interrupts per interval. A high **cs** or **in** rate may be an indication to a malunctioning hardware device.</span><br><span class="line"></span><br><span class="line">**CPU load average and per-process**</span><br><span class="line">- Using the **uptime** command, it essentially provides the total time spent since the system was booted, but it also offers a CPU load average for the same period.</span><br><span class="line">- The load average consists of three vlues that represent 5,10, and 15 minutes averages.</span><br><span class="line">- A load average that stays the same on a &quot;good performance&quot; and on a &quot;performance degraded&quot; one is an indication that you have to look elsewhere,perhaps at the network bandwidth, disk I/O, or the intalled memroy.</span><br><span class="line">- Other commands that offer real time view of the CPU per-process load is **ps -aux** and **top**. You may find a single process using more than 50% of the available CPU time. Using **nice** to decrease the execution prioroty of this process may help boost performance.</span><br><span class="line"></span><br><span class="line">**Memeory management**</span><br><span class="line">- When an application requests memeory to operate, the kernel offers this memeory in the form of &quot;pages&quot;. In linux, a page size is 4KiB.</span><br><span class="line">- The kernel serves those pages from physical storage hardware (either RAM or SWAP space on the disk).</span><br><span class="line">- The kernel shuffles pages between the SWAP space together with RAM. Memroy that is not accessed for a specific period of time is moved into SWAP space (paged) to free more space for rather more frequently accessed memory.</span><br><span class="line">- As more and more processes demand memroy, the kernel tries to fulfil the reqeusts by paging in and out memory pages from and to the SWAP space. And because the disk is the slowest coponent of the system, as the paging rate increates, performance is degraded as processes will have to wait longer before they can have their requested memory and things start to get slower.</span><br><span class="line">- Fainlly, if the system runs out of both physical memory and SWAP space, the kernel resorts to more drastic measures: it kills the least important process with an out-of-memory killer function, a situation that should be avoided at all costs by anticipating the need to install more memroy early enough.</span><br><span class="line"></span><br><span class="line">**Using vmstat to measure memory utilization**</span><br><span class="line">- **vmstat** is used the same way it was used to measure CPU utilization.</span><br><span class="line">- The swap in (si) and swap out (so) columns in the SWAP area of the output are of the most importance here. Pages that are read from disk into memory are &quot;swapped in&quot; while those which are ejected by the kernel into the disk are &quot;swapped out&quot;. A high rate of si and so may be an indication that the system is using SWAP sapce extensively and that it might need more physical memeory to be installed.</span><br><span class="line">- Such a decision should not be reached by the **si** and **so** rates alone as the system normally does page in and page out operations. Only if is accompanied by slow system response and user complaints.</span><br></pre></td></tr></table></figure></p><p>iostat -dx 5 5<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">**A slow system quick diagnosis and remedy**</span><br><span class="line">- If you find that the system is suddenly running slower than before and users start complaining, you can examine the resources discussed in this section for bottlenecks.</span><br><span class="line">- For example, running **ps -auxww** will show you the CPU utilization per process. If you find that a single process is using more than 50% of the CPU ofr a long time, this might be an indication of fault in the process itself. Also check the load average with uptime to determine whether or not the CPU is contended.</span><br><span class="line"></span><br><span class="line">- Check the paging activity with vmstat. If there are a lot of page-outs this means the physical memeory is overloaded. Additionalyy, if there is a lot of disk activity without paging this means the a process is extensively using the disk for read and write requests. If this is not the normal behavior (e.g. a database), the process activity should be further examined.</span><br><span class="line">- It is difficult to know exactly which process is using the disk I/O the most, but using kill -STOP to temporarily suspend the susceptiable process can narrow down the possibilities.</span><br><span class="line">- If a process is identified as resource intensive, a number of actions can be taken: if it is CPU intensive you can use the renice command to descrease its priority. You can also ask the user to run it later. I the process is hogging the disk and/or then network, renice will not solve the problem, but you can tune the process itself to optimize its behavior (for example web servers).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Using screen, you can start a session that’s tied to a single operation. Then you can connect or disconnect whenever you want, and come back to the session to check on its progress.</span><br><span class="line">``screen</span><br><span class="line">screen -r</span><br><span class="line">screen -ls</span><br><span class="line">screen -r num</span><br><span class="line">```</span><br></pre></td></tr></table></figure></p><p><code>`</code></p><h1 id="Linux-troubleshooting"><a href="#Linux-troubleshooting" class="headerlink" title="Linux troubleshooting"></a>Linux troubleshooting</h1><h2 id="System-access-troubleshooting"><a href="#System-access-troubleshooting" class="headerlink" title="System access troubleshooting"></a>System access troubleshooting</h2><p><strong>Server is not reachable</strong></p><ul><li>Ping the destination server name<ul><li>if server name is not pingable</li></ul></li><li>Ping the destination server yb IP<ul><li>if IP is pingable = Name resolution issue<ul><li>Check /etc/hosts file</li><li>Check /etc/resolv.conf</li><li>Check /etc/nsswitch.conf</li></ul></li><li>If IP is NOT pingable<ul><li>Ping another server by name and then by IP</li><li>Checking if your server has an IP address</li><li>Ping your gateway (netstat -rnv)/modem IP</li><li>Check physical cable connection</li></ul></li></ul></li></ul><p><strong>Cannot connect to a website or an application</strong><br>Troubleshoting Steps: ping server by hostname and IP<br>if NOT pinable= go back to <strong>Server is not reachable</strong></p><p>if pingable = Connect to service </p><h1 id="telnet-192-168-1-5-80-http"><a href="#telnet-192-168-1-5-80-http" class="headerlink" title="telnet 192.168.1.5 80 (http)"></a>telnet 192.168.1.5 80 (http)</h1><p>Verify the installation:</p><h1 id="rpm-q-haproxy"><a href="#rpm-q-haproxy" class="headerlink" title="rpm -q haproxy"></a>rpm -q haproxy</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;TCP-IP-networking&quot;&gt;&lt;a href=&quot;#TCP-IP-networking&quot; class=&quot;headerlink&quot; title=&quot;TCP/IP networking&quot;&gt;&lt;/a&gt;TCP/IP networking&lt;/h1&gt;&lt;h2 id=&quot;Troub
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://223.95.78.227/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>programming4sysadmins</title>
    <link href="http://223.95.78.227/2019/07/19/programming4sysadmins/"/>
    <id>http://223.95.78.227/2019/07/19/programming4sysadmins/</id>
    <published>2019-07-19T06:10:53.000Z</published>
    <updated>2019-07-19T14:27:15.327Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><h2 id="What-can-you-do-with-python"><a href="#What-can-you-do-with-python" class="headerlink" title="What can you do with python?"></a>What can you do with python?</h2><ul><li>Versatile language, can use it across a lot of different domains</li><li>really fast to learn and fast to develop in<h2 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h2><a href="https://wiki.openhatch.org/wiki/O%27Reilly_Introduction_to_Python">link</a><h2 id="Python-basic-data-types"><a href="#Python-basic-data-types" class="headerlink" title="Python basic data types"></a>Python basic data types</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; type(1)</span><br><span class="line">&lt;type &apos;int&apos;&gt;</span><br><span class="line">&gt;&gt;&gt; type(1.0)</span><br><span class="line">&lt;type &apos;float&apos;&gt;</span><br><span class="line">Type is a functions that takes input and spits out output.</span><br></pre></td></tr></table></figure></li></ul><p><strong>a function</strong> is just the name and then input inside parentheses, and it’ll spit out output.</p><p>If you need to include the string delimiter inside the string just precede it with a backslash, as in ‘It\’s a wrap’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; &quot;Hello &quot; + str(1)</span><br><span class="line">&apos;Hello 1&apos;</span><br></pre></td></tr></table></figure></p><h2 id="Making-choices-boolean-if-eif-else-compound-conditionals"><a href="#Making-choices-boolean-if-eif-else-compound-conditionals" class="headerlink" title="Making choices: boolean, if/eif/else, compound conditionals"></a>Making choices: boolean, if/eif/else, compound conditionals</h2><p>Boolean:</p><ul><li>True</li><li>False<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = 1</span><br><span class="line">&gt;&gt;&gt; x &gt; 0 and x &lt; 2</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; &quot;a&quot; in &quot;hello&quot; or &quot;e&quot; in &quot;hello&quot;</span><br><span class="line">True</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; temp = 32</span><br><span class="line">&gt;&gt;&gt; if temp &gt; 60 and temp &lt; 75:</span><br><span class="line">...     print(&quot;Nice and cozy&quot;)</span><br><span class="line">... else:</span><br><span class="line">...     print(&quot;Too extreme for me&quot;)</span><br><span class="line">...</span><br><span class="line">Too extreme for me</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; sister = 15</span><br><span class="line">&gt;&gt;&gt; brother =15</span><br><span class="line">&gt;&gt;&gt; if sister &gt; brother:</span><br><span class="line">...     print(&quot;Sister is older&quot;)</span><br><span class="line">... elif sister == brother:</span><br><span class="line">...     print(&quot;Same age!&quot;)</span><br><span class="line">... else:</span><br><span class="line">...     print(&quot;Brother is older&quot;)</span><br><span class="line">...</span><br><span class="line">Same age!</span><br></pre></td></tr></table></figure></li></ul><h2 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; names = [&quot;Alice&quot;, &quot;Amy&quot;]</span><br><span class="line">&gt;&gt;&gt; names.append(&quot;Adam&quot;)</span><br><span class="line">&gt;&gt;&gt; names</span><br><span class="line">[&apos;Alice&apos;, &apos;Amy&apos;, &apos;Adam&apos;]</span><br><span class="line">&gt;&gt;&gt; names[len(names)-1]</span><br><span class="line">&apos;Adam&apos;</span><br><span class="line">&gt;&gt;&gt; names[-1]</span><br><span class="line">&apos;Adam&apos;</span><br></pre></td></tr></table></figure><p>The real superpower when using lists is actually to be able to loop over them.</p><h2 id="Loops"><a href="#Loops" class="headerlink" title="Loops"></a>Loops</h2><h1 id="Great-Bash"><a href="#Great-Bash" class="headerlink" title="Great Bash"></a>Great Bash</h1><ul><li><p>Redirect the output of commands</p><ul><li>Standard error is file descriptor “2” <code>ls -l myscript not.here &gt; lsout 2&gt; lserr</code></li><li>Out and error can be redirected separately or together <code>ls -l myscript not.here &amp;&gt; lsboth</code></li><li>The order of redirection is important<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls -l myscript not.here &gt; lsout 2&gt;&amp;1</span><br><span class="line">## Redirectin error output to standard output</span><br><span class="line">## Standard output is already being re-directed to file &gt; dirlist</span><br><span class="line">## Hence, both error and standard output are written to file lsout</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Redirecting and piping input and output</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls &gt; /tmp/lsout</span><br><span class="line">wc &lt; /tmp/lsout</span><br></pre></td></tr></table></figure></li></ul><p>Use the vertical bar character | to create a pipe: <code>ls|wc</code></p><p>Connect a series of commands with | symbols to make a pipeline.</p><ul><li>Create input with here documents</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;python&quot;&gt;&lt;a href=&quot;#python&quot; class=&quot;headerlink&quot; title=&quot;python&quot;&gt;&lt;/a&gt;python&lt;/h1&gt;&lt;h2 id=&quot;What-can-you-do-with-python&quot;&gt;&lt;a href=&quot;#What-can-y
      
    
    </summary>
    
    
      <category term="python" scheme="http://223.95.78.227/tags/python/"/>
    
      <category term="programming" scheme="http://223.95.78.227/tags/programming/"/>
    
      <category term="systemadmin" scheme="http://223.95.78.227/tags/systemadmin/"/>
    
  </entry>
  
  <entry>
    <title>DevOps</title>
    <link href="http://223.95.78.227/2019/07/17/devops/"/>
    <id>http://223.95.78.227/2019/07/17/devops/</id>
    <published>2019-07-17T11:23:02.000Z</published>
    <updated>2019-09-13T14:59:52.588Z</updated>
    
    <content type="html"><![CDATA[<p><strong>What is DevOps?</strong></p><ul><li>Speed and agility enable organizations to better serve their customers and compete more effectively in the market</li><li>Combination of cultural philosophies, practices, and tools</li><li>Increases an organization’s ability to deliver applications and services at high velocity</li><li>Evolves and imprvoes products faster</li></ul><p><strong>Why DevOps?</strong></p><ul><li>Antomate manual tasks, help teams manage complex environments at scale, and keep engineers in control of the velocity that is enabled by DevOps:<ul><li>Speed</li><li>Rapid delivery</li><li>Reliability</li><li>Scale</li><li>Improved collaboration</li><li>Security</li></ul></li></ul><p>Standard Continuous delivery (CD) techniques</p><ul><li>Blue/Green deployment (where “live” and “last” deployments are maintained on live)<br>Blue-green deployment is a technique that reduces downtime and risk by running two identical production environments called Blue and Green.</li></ul><p>At any time, only one of the environments is live, with the live environment serving all production traffic. For this example, Blue is currently live and Green is idle.</p><p>As you prepare a new version of your software, deployment and the final stage of testing takes place in the environment that is not live: in this example, Green. Once you have deployed and fully tested the software in Green, you switch the router so all incoming requests now go to Green instead of Blue. Green is now live, and Blue is idle.</p><p>This technique can eliminate downtime due to app deployment. In addition, blue-green deployment reduces risk: if something unexpected happens with your new version on Green, you can immediately roll back to the last version by switching back to Blue.</p><ul><li>Phoenix deployment (where whole system are rebuilt on each release).<br><img src="https://i.imgur.com/DXtJZOm.png" alt="devops tools"></li></ul><p><strong>Goals</strong></p><ul><li>Culture for collaboration<blockquote><p>lack of collaboration is one of the root causes of the issues</p></blockquote></li><li>Automate<blockquote><p>Manual tests that consume resources are better left to machines. This frees time that is better spent elsewhere, and provides a better environment by relinquishing mundane tasks</p></blockquote></li><li>Optimize and reduce issue in SDLC(software development life cycle)<blockquote><p>The processes being comprised in a logical order allows for optimizations to be made from recorded metrics.</p></blockquote></li><li>Consistency in process<blockquote><p>This stems mostly from automation, and provides the foundation to ensure quality. It also provides a certain level of peace of mind, having confidence that the same process that successfully ran last time will run the same the next.</p></blockquote></li><li>Improve quality and security<blockquote><p>Automation combined with consistency in process, along with the tools and practices in place to perform the necessary testing and scans, removes the possibility of human error from subsequent processes</p></blockquote></li><li>Improve deployment frequency<blockquote><p>Agile methodologies proved effective for development, and sparked the idea to apply similar principles to other areas. Deployment frequency has always been a target for efficiency, as shown by the migration to waterfall, where deployments were seldom; to hybrid methodologies that produced releases four or five times per month; to agile, where deployments are dependent upon sprint links. With DevOps, there’s a potential to release multiple times per day.</p></blockquote></li></ul><p><strong>DevOps Methodologies and Concepts</strong></p><ul><li>Automation<ul><li>Not duplicate of goals</li><li>Automation in context of applying automation to deveopment, integration, and deployment process</li></ul></li><li>CI/CD/CD<ul><li>CI: continuous integration<ul><li>Focusses on sub process of SDLC to build features and fixes perform preliminary testing then merge to master if successful</li></ul></li><li>CD: continuous delivery<ul><li>tail end of CI</li><li>refers to archiving build artifacts</li></ul></li><li>CD: continous deployment<ul><li>deploy all necessary artifacts and perform any configuration</li></ul></li></ul></li><li>Infrastructure as code, configuration as code: fail fast<ul><li>Don’t waste resources on something that will fail</li><li>Organize and optimize for efficiency</li></ul></li><li>Frequent feedback<ul><li>Compilation of a series of small and frequent feedback loops</li></ul></li></ul><p><strong>DevOps engineer’s role within a devops organization</strong></p><ul><li>Continually research, refine, and create conceopts, methodologies, practices, and tools in order to optimize the SDLC.</li><li>Implement core standards, plicies, and tools based on the previous</li><li>Asses infrastructure requirements”<ul><li>Global and application sacle</li></ul></li><li>Crate and manage infrastructure</li><li>Coordinate with other teams</li><li>Troubleshoot issues with code and infrastructure</li><li>Work with one or more teams to:<ul><li>Assess their current state</li><li>Formulate end goals</li><li>Adapt to requirements</li><li>Develop a plan of implementation</li><li>Formulate milestones</li><li>Provide instruction on the core standards, policies, and tools</li><li>Develop a pipeline</li><li>Help to change their code and processes to work with the plan</li></ul></li></ul><p><strong>philosophy</strong><br>DevOps is not something you do. It is something you are. </p><p>Devops culture is the one based on a set of principles, hierarchy of rules, by which each person operates.<br>A DevOps culture is one that allows more freedom but more responsibility.</p><p><strong>Devops lifecycle</strong></p><ul><li><p>Continuous integration</p><ul><li>Central repository</li><li>Continuous compiling, testing</li><li>Code verification</li><li>Identifying bugs early</li><li>Software in smaller chunks</li><li>Easy integration<br>Continuous integration (CI) requires developers to integrate code into a centralized repository as they finish coding and successfully pass unit testing, several times per day. The end goal is to create small workable chunks of code that are validated and integrated back into the code repository as frequently as possible. </li></ul></li><li><p>Configuration management</p><ul><li>System changes</li><li>Multiple servers</li><li>Tracking changes</li><li>Types of CM tools</li><li>Scripted builds</li><li>Identical development and production environments</li></ul></li><li><p>Continous delivery</p><ul><li>Deploying the application</li><li>Incremental or small changes</li><li>Compatible with schedule releases</li><li>Every change-ready to deploy<br>Continuous delivery simply means that every change is ready to be deployed to production as soon as automated testing validates it.<br>Note there are two manual checkpoints in this example. One is for a technical decision to approve the code before initiating activities in the CI environment. The second is a business decision to accept the changes and continue with the automated steps to production deployment.<br><img src="https://i.imgur.com/XzLyd0g.png" alt="reference pipeline-continous delivery"><br><strong>Deploying to production</strong></li></ul></li><li><p>Canary releases</p><blockquote><p>Continuous delivery deploys many builds to production. In a canary deployment, the new code is delivered only to a percentage of the existing infrastructure. For example, if the system is running on 10 load-balanced virtual servers, you can define a canary cluster of one or two servers. This way, if the deployment is not successful due to an escaped defect, it can be caught before the build is deployed to all of the servers. Canary releases are also used for pilot features to determine performance and acceptance prior to a full rollout.</p></blockquote></li><li><p>Blue/green deployment</p><blockquote><p>This is a zero-downtime deployment technique that involves a gradual release to ensure uninterrupted service. The blue/green approach is effective in virtualized environments, especially if IaaS is used. Although a blue/green deployment is a deep topic that deserves an entire chapter on its own, simply put, it includes maintaining two identical development environments—Blue and Green. One is a live environment for production traffic, whereas the other is used to deploy the new release. In our example, let’s say that Green is the current live production environment and Blue is the idle identical production environment. After the code is deployed and tested in the Blue environment, we can begin directing traffic of incoming requests from Green (current production) to Blue. You can do this gradually until the traffic redirect is 100 percent to the Blue environment. If unexpected issues occur during this gradual release, we can roll back. When it is completed, the Green environment becomes idle and the Blue environment is now the live production environment.</p></blockquote></li><li><p>Continous monitoring<br>Continous monitoring is the practice that connects operations back to development,providing visibility and relevant data throughout the development lifecycle including production monitoring. continuous monitoring aims to reduce the time between identification of a problem and deployment of the fix.<br>Monitoring begins with <strong>Sprint 1</strong> and should be integrated into the development work. As the system is built, monitoring solutions are also designed. </p></li></ul><p><strong>four different types of continuous monitoring</strong></p><ul><li>Infrastructure monitoring<blockquote><p>Visualize infrastructure events coming from all computing resources, storage and network, and measure the usage and health of infrastructure resources. <strong>AWS CloudWatch</strong> and <strong>CloudTrail</strong> are examples of infrastructure monitoring tools.</p></blockquote></li><li>Application performance monitoring (APM)<blockquote><p>Target bottlenecks in the application’s framework. <strong>Appdynamics</strong> and <strong>New Relic</strong> are industry-leading APM tools.</p></blockquote></li><li>Log management monitoring<blockquote><p>Collect performance logs in a standardized way and use analytics to identify application and system problems. <strong>Splunk</strong> and <strong>ELK</strong> are two leading products in this area.</p></blockquote></li><li>Security monitoring<blockquote><p>Reduce security and compliance risk through automation. Security configuration management, vulnerability management, and intelligence to detect attacks and breaches before they do serious damage are achieved through continuous monitoring. For example, <strong>Netflix’s Security Monkey</strong> is a tool that checks the security configuration of your cloud implementation on AWS.</p></blockquote></li></ul><p><strong>three major setps</strong></p><ol><li>monitoring</li><li>an alert system to warn the team about a problem</li><li>actions to take when an alert occurs</li></ol><ul><li>Continous testing<ul><li>Speed and quality</li><li>Testing incremental changes</li><li>Automated tests</li><li>Tests to be atomic: small test<ul><li>Continous testing during development (e.g. use open source tools like Selenium for testing)</li><li>Confidence to release</li><li>Integration with CI</li></ul></li></ul></li><li>Continous deployment<ul><li>Superset of Continuous Delivery</li><li>Deploying to production</li><li>Automates the deployment pipeline<br>Unlike continuous delivery, which means that every change is deployable but might be held back because of business considerations or other manual steps, continuous deployment strives to automate production deployment end to end. With this practice, confidence in the automated tests is extremely high, and as long as the code has passed all the tests, it will be deployed.<br><img src="https://i.imgur.com/Aq1VaJb.png" alt="cd"></li></ul></li></ul><p>DevOps: culture, automation, measurement, and sharing (CAMS)</p><p><strong>DevOps architecutres and practics</strong><br>From the DevOps movement, a set of software architectural patterns and practices have become increasingly popular. The primary logic behind the development of these architectural patterns and practices is derived from the need for scalability, no-downtime deployments, and minimizing negative customer reactions to upgrades and releases. Some of these you may have heard of (microservices), while others may be a bit vague (blue-green deployments).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;What is DevOps?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Speed and agility enable organizations to better serve their customers and compete more eff
      
    
    </summary>
    
    
      <category term="Devops" scheme="http://223.95.78.227/tags/Devops/"/>
    
  </entry>
  
  <entry>
    <title>aws administration</title>
    <link href="http://223.95.78.227/2019/07/17/aws-admin/"/>
    <id>http://223.95.78.227/2019/07/17/aws-admin/</id>
    <published>2019-07-17T08:02:52.000Z</published>
    <updated>2019-07-20T07:03:14.782Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CodeCommit"><a href="#CodeCommit" class="headerlink" title="CodeCommit"></a>CodeCommit</h1><p>Codecommit is a source control service provided by aws(hosts private git repositories)<br><strong>why choose CodeCommit</strong></p><ul><li>Easy integration with other AWS services like CodePipeline</li><li>Repositories are private by default</li><li>Can use IAM for fine-graned authorization</li></ul><p><strong>Creating a private code repo on CodeCommit</strong><br>IAM-&gt; HTTPS Git credentials for AWS CodeCommit</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git clone https://git-codecommit.ap-southeast-2.amazonaws.com/v1/repos/SampleRepo</span><br><span class="line">cd SampleRepo</span><br><span class="line">wget https://s3.amazonaws.com/aws-codedeploy-us-east-1/samples/latest/SampleApp_Linux.zip</span><br><span class="line">unzip SampleApp_Linux.zip</span><br><span class="line">rm SampleApp_Linux.zip</span><br><span class="line">git add -A</span><br><span class="line">git commit -m &quot;Initial commit&quot;</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p><strong>Migrating your project to CodeCommit</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">### - - mirror here means that we are not interested in cloning the application, however we are interested in downloading the Git binary files that make up the repository in the first place. </span><br><span class="line">git clone --mirror https://github.com/awslabs/aws-demo-php-simple-app.git aws-codecommit-demo</span><br><span class="line">git push https://git-codecommit.ap-southeast-2.amazonaws.com/v1/repos/myrepo</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;initial commit to demp rep&quot;</span><br><span class="line">git remote add demo https://git-codecommit.ap-southeast-2.amazonaws.com/v1/repos/demo</span><br><span class="line">git remote -v</span><br><span class="line">git push demo master</span><br></pre></td></tr></table></figure><p><a href="https://docs.aws.amazon.com/codecommit/latest/userguide/getting-started-cc.html">CodeCommit tutorials</a><br><a href="https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-iam-identity-based-access-control.html">Using IAM policies with CodeCommit</a><br><a href="https://aws.amazon.com/blogs/devops/using-aws-codecommit-pull-requests-to-request-code-reviews-and-discuss-code/">Using AWS CodeCommit Pull Requests</a><br><a href="https://datasift.github.io/gitflow/IntroducingGitFlow.html">GitFlow development release model</a></p><h1 id="Deploying-Jenkins"><a href="#Deploying-Jenkins" class="headerlink" title="Deploying Jenkins"></a>Deploying Jenkins</h1><p>Amazon linux 2 AMI<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y update</span><br><span class="line">sudo yum -y install java-1.8.0</span><br><span class="line"># remove java 7</span><br><span class="line">sudo yum remove java-1.7.0-openjdk</span><br><span class="line">java -version</span><br><span class="line">sudo wget -O /etc/yum.repos.d/jenkins.repo https:/pkg.jenkins-ci.org/redhat/jenkins.repo </span><br><span class="line">sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key</span><br><span class="line">sudo yum -y install jenkins</span><br><span class="line">sudo service jenkins start</span><br></pre></td></tr></table></figure></p><h1 id="CodeDeploy"><a href="#CodeDeploy" class="headerlink" title="CodeDeploy"></a>CodeDeploy</h1><p><img src="https://i.imgur.com/ZRCFrZP.png" alt="codedeploy ec2"></p><h1 id="Cloud9"><a href="#Cloud9" class="headerlink" title="Cloud9"></a>Cloud9</h1><p><strong>What is Cloud9</strong></p><ul><li>Cloud9 integrates with other AWS DevOps tools such as CodeCommit, CodePipeline, and CodeStar to enable a rich development pipeline with continuous delivery</li><li>AWS Cloud9 contains a collection of tools that you use to code, build, run, test, debug, and release software on the cloud</li><li>Use the AWS Cloud9 Integrated Development Environment (IDE) to work with these tools<br><strong>What can i do with aws Cloud 9</strong></li><li>Supported languages:<ul><li>C++</li><li>Java</li><li>Python</li><li>.NET</li><li>Node.js</li><li>PHP</li><li>Pearl</li><li>Ruby</li><li>Go</li><li>JavaScript</li><li>CoffeeScript</li></ul></li><li>Supported integrations:<ul><li>CodeCommit</li><li>CodePipeline</li><li>CodeStar</li><li>API gateway</li><li>Lambda</li><li>Lightsail</li><li>DynamoDB</li><li>RDS</li><li>AWS CLI</li><li>Docker</li><li>GitHub</li></ul></li><li>Supported environments<ul><li>EC2</li><li>SSH</li><li>Single-user environment</li><li>Shared team environment</li><li>virtualenv<h1 id="CodeBuild"><a href="#CodeBuild" class="headerlink" title="CodeBuild"></a>CodeBuild</h1><h1 id="CodePipeline"><a href="#CodePipeline" class="headerlink" title="CodePipeline"></a>CodePipeline</h1><h1 id="CodeStart"><a href="#CodeStart" class="headerlink" title="CodeStart"></a>CodeStart</h1></li></ul></li></ul><h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><ul><li><a href="https://aws.amazon.com/devops/what-is-devops/">What is DevOps</a></li><li><a href="https://aws.amazon.com/pricing/">AWS pricing</a></li><li><a href="https://docs.aws.amazon.com/codepiepline/latest/userguide/tutorials-simple-codecommit.html">TUtorial-create a simple pipeline</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CodeCommit&quot;&gt;&lt;a href=&quot;#CodeCommit&quot; class=&quot;headerlink&quot; title=&quot;CodeCommit&quot;&gt;&lt;/a&gt;CodeCommit&lt;/h1&gt;&lt;p&gt;Codecommit is a source control service
      
    
    </summary>
    
    
      <category term="aws" scheme="http://223.95.78.227/tags/aws/"/>
    
      <category term="Codecommit" scheme="http://223.95.78.227/tags/Codecommit/"/>
    
  </entry>
  
  <entry>
    <title>devops-with-aws</title>
    <link href="http://223.95.78.227/2019/07/15/devops-with-aws/"/>
    <id>http://223.95.78.227/2019/07/15/devops-with-aws/</id>
    <published>2019-07-15T13:44:53.000Z</published>
    <updated>2019-08-13T01:08:49.426Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Project-setup"><a href="#Project-setup" class="headerlink" title="Project setup"></a>Project setup</h1><p><a href="https://github.com/espiderinc/hapi-rest-demo">code</a></p><h2 id="Importance-of-automated-test-in-CI-CD"><a href="#Importance-of-automated-test-in-CI-CD" class="headerlink" title="Importance of automated test in CI,CD"></a>Importance of automated test in CI,CD</h2><ul><li>Automated tests<ul><li>Unit tests</li><li>Integration tests</li><li>UAT tests</li></ul></li><li>Code coverage</li><li>Notifications<h2 id="CI-CD-with-relational-databases"><a href="#CI-CD-with-relational-databases" class="headerlink" title="CI/CD with relational databases"></a>CI/CD with relational databases</h2></li><li>Managing the version of database schema<br>There is no easy way to control the version of relational database schema</li><li>Database schema migrations</li><li>DellStore2 sample database<ul><li>Products table</li></ul></li><li>Sqitch change management system<h2 id="Project-component-setup"><a href="#Project-component-setup" class="headerlink" title="Project component setup"></a>Project component setup</h2></li><li>PostgreSQL database on AWS RDS</li><li>Node.JS HAPI RESTful API project</li><li>Sqitch database mangement framework<h2 id="Setup-PostreSQL-database-instance-in-AWS-RDS"><a href="#Setup-PostreSQL-database-instance-in-AWS-RDS" class="headerlink" title="Setup PostreSQL database instance in AWS RDS"></a>Setup PostreSQL database instance in AWS RDS</h2></li></ul><ol><li>Create a rds in aws with postgresql engine version 9.4.7.</li><li>Connect to aws rds use pgAdmin 3.</li><li>Download sample schema dellstore2 from <a href="http://pgfoundry.org/projects/dbsamples/">link</a></li><li>In pgAdmin3 click Plugins-&gt; PSQL console-&gt; run command <code>\i /tmp/dellstore2.sql</code> to create a new schema.<h2 id="Setup-Node-JS-HAPI-ReSTful-API-project"><a href="#Setup-Node-JS-HAPI-ReSTful-API-project" class="headerlink" title="Setup Node.JS HAPI ReSTful API project"></a>Setup Node.JS HAPI ReSTful API project</h2><strong>HAPI</strong> is a rich application framework for building applications and RESTful APIs with Node.JS</li></ol><p>Official website for HAPI framework is HAPIJS.com</p><ol><li>install node and npm<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line">mkdir myfirsthapiproject</span><br><span class="line">cd myfirsthapiproject</span><br><span class="line">npm init</span><br><span class="line">npm install --save hapi</span><br></pre></td></tr></table></figure></li></ol><p>2.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/espiderinc/hapi-rest-demo.git</span><br><span class="line">cd hapi-rest-demo</span><br><span class="line">npm install</span><br><span class="line">sudo npm install -g istanbul mocha</span><br><span class="line">node index.js</span><br></pre></td></tr></table></figure></p><ol start="3"><li><p><img src="https://i.imgur.com/rCcEgzW.png" alt="access by">)<br><img src="https://i.imgur.com/VdGe8f3.png" alt="Imgur"></p></li><li><p>test</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hapi-rest-demo git:(master) ✗ npm test</span><br><span class="line"></span><br><span class="line">&gt; hapi-rest-demo@1.0.0 test /home/stan/workspace/hapi-rest-demo</span><br><span class="line">&gt; istanbul cover _mocha test/**/*.js</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(node:20777) [DEP0022] DeprecationWarning: os.tmpDir() is deprecated. Use os.tmpdir() instead.</span><br><span class="line">  Task routes</span><br><span class="line">    GET /products</span><br><span class="line">      ✓ should return statusCode 200 (381ms)</span><br><span class="line">      ✓ should return product [ACADEMY BROOKLYN] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  2 passing (416ms)</span><br><span class="line"></span><br><span class="line">=============================================================================</span><br><span class="line">Writing coverage object [/home/stan/workspace/hapi-rest-demo/coverage/coverage.json]</span><br><span class="line">Writing coverage reports at [/home/stan/workspace/hapi-rest-demo/coverage]</span><br><span class="line">=============================================================================</span><br><span class="line"></span><br><span class="line">=============================== Coverage summary ===============================</span><br><span class="line">Statements   : 56.31% ( 58/103 )</span><br><span class="line">Branches     : 39.29% ( 11/28 )</span><br><span class="line">Functions    : 47.83% ( 11/23 )</span><br><span class="line">Lines        : 57% ( 57/100 )</span><br><span class="line">================================================================================</span><br></pre></td></tr></table></figure></li></ol><p>report generated workspace/hapi-rest-demo/coverage/lcov-report/index.html</p><h2 id="Setup-swtich-database-schema-framework"><a href="#Setup-swtich-database-schema-framework" class="headerlink" title="Setup swtich (database schema framework)"></a>Setup swtich (database schema framework)</h2><p>Managin database schema for relational databaes (with Sqitch)<br><strong>Sqitch</strong> is a standalone system without any dependency on frameworks or ORMs.</p><ul><li>handels dependencies between scripts</li><li><a href="http://sqitch.org">project site</a><br><strong>install sqitch</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull sqitch/sqitch</span><br><span class="line">curl -L https://git.io/fAX6Z -o sqitch &amp;&amp; chmod +x sqitch</span><br><span class="line">mv sqitch /usr/bin/</span><br><span class="line">sudo apt-get install -y libdbd-pg-perl postgresql-client</span><br><span class="line">sqitch --version</span><br></pre></td></tr></table></figure></li></ul><p><strong>use sqitch</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">mkdir stantutorial</span><br><span class="line">cd stantutorial</span><br><span class="line">sqitch init stantutorial --uri http://github.com/espiderinc/hapi-rest-demo.git</span><br><span class="line">cat sqitch.conf</span><br><span class="line"> [core]</span><br><span class="line">         engine = pg</span><br><span class="line">        # plan_file = sqitch.plan</span><br><span class="line">        # top_dir = .</span><br><span class="line">sqitch config --user user.name &apos;StanTutorial&apos;</span><br><span class="line">sqitch config --user user.email &apos;devops@cwzhou.win&apos;</span><br><span class="line">sqitch add schema -n &apos;Add schema for tutorial objects.&apos;</span><br><span class="line">Created deploy/schema.sql</span><br><span class="line">Created revert/schema.sql</span><br><span class="line">Created verify/schema.sql</span><br><span class="line">Added &quot;schema&quot; to sqitch.plan</span><br><span class="line"></span><br><span class="line"> cat deploy/schema.sql</span><br><span class="line">-- Deploy stantutorial:schema to pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add DDLs here.</span><br><span class="line">CREATE SCHEMA tutorial;</span><br><span class="line"></span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">cat revert/schema.sql</span><br><span class="line">-- Revert stantutorial:schema from pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add DDLs here.</span><br><span class="line">DROP SCHEMA tutorial;</span><br><span class="line"></span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line"> cat verify/schema.sql</span><br><span class="line">-- Verify stantutorial:schema on pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add verifications here.</span><br><span class="line"></span><br><span class="line">select 1/count(*) from information_schema.schemata where schema_name=&apos;tutorial&apos;;</span><br><span class="line"></span><br><span class="line">ROLLBACK;</span><br><span class="line"></span><br><span class="line"> sqitch add video --requires schema -n &apos;add video table to schema tutorial&apos;</span><br><span class="line"></span><br><span class="line"> cat deploy/video.sql</span><br><span class="line">-- Deploy stantutorial:video to pg</span><br><span class="line">-- requires: schema</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add DDLs here.</span><br><span class="line">SET client_min_messages = &apos;warning&apos;;</span><br><span class="line">CREATE TABLE tutorial.video(</span><br><span class="line">        subject TEXT    PRIMARY KEY,</span><br><span class="line">        comment TEXT,</span><br><span class="line">        timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW()</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">cat revert/video.sql</span><br><span class="line">-- Revert stantutorial:video from pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add DDLs here.</span><br><span class="line">DROP TABLE tutorial.video;</span><br><span class="line"></span><br><span class="line">COMMIT;</span><br><span class="line"></span><br><span class="line">cat verify/video.sql</span><br><span class="line">-- Verify stantutorial:video on pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add verifications here.</span><br><span class="line">select subject, comment, timestamp</span><br><span class="line">from tutorial.video</span><br><span class="line">where false;</span><br><span class="line"></span><br><span class="line">ROLLBACK;</span><br><span class="line">sqitch deploy db:pg://foo:bar@stantest-postgresql.c3mzoji03zxf.ap-southeast-2.rds.amazonaws.com:5432/postgres</span><br><span class="line">sqitch status db:pg://foo:bar@stantest-postgresql.c3mzoji03zxf.ap-southeast-2.rds.amazonaws.com:5432/postgres</span><br><span class="line">sqitch verify db:pg://foo:bar@stantest-postgresql.c3mzoji03zxf.ap-southeast-2.rds.amazonaws.com:5432/postgres</span><br><span class="line">sqitch revert db:pg://foo:bar@stantest-postgresql.c3mzoji03zxf.ap-southeast-2.rds.amazonaws.com:5432/postgres</span><br><span class="line">sqitch status db:pg://foo:bar@stantest-postgresql.c3mzoji03zxf.ap-southeast-2.rds.amazonaws.com:5432/postgres</span><br></pre></td></tr></table></figure></p><h1 id="CI-and-CD-pipeline-deep-dive"><a href="#CI-and-CD-pipeline-deep-dive" class="headerlink" title="CI and CD pipeline deep dive"></a>CI and CD pipeline deep dive</h1><h2 id="AWS-prerequisites"><a href="#AWS-prerequisites" class="headerlink" title="AWS prerequisites"></a>AWS prerequisites</h2><ul><li>IAM instance profile</li></ul><ol><li><p>Create a policy, name: CodeDeploy-EC2-Permissions, json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">     &#123;</span><br><span class="line">        &quot;Action&quot;:[</span><br><span class="line">           &quot;s3:Get*&quot;,</span><br><span class="line">           &quot;s3:List*&quot;  </span><br><span class="line">        ],</span><br><span class="line">        &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">        &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">     &#125;    </span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Create a role, named CodeDeploy-EC2 -&gt; Choose role type ec2-&gt;Attach permissions policies “CodeDeploy-EC2-Permissions”</p></li></ol><ul><li>IAM service role</li></ul><ol><li>Create a role named stantutorialRole -&gt; select role type CodeDeploy</li></ol><h2 id="Jenkins-installation"><a href="#Jenkins-installation" class="headerlink" title="Jenkins installation"></a>Jenkins installation</h2><p>Ubuntu-&gt; Configure Instance Details, IAM role, select CodeDeploy-EC2 (this will allow jenkins connect to s3 buckets)-&gt;<br>Tag instance: Key group Value hapi-demo<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget -q -O - http://pkg.jenkins-ci.org/debian/jenkins-ci.org.key| sudo apt-key add -</span><br><span class="line">sudo vim /etc/apt/sources.list # add following line</span><br><span class="line">deb http://pkg.jenkins-ci.org/debian-stable binary/</span><br><span class="line">sudo apt-get install default-jdk</span><br><span class="line">sudo apt-get install jenkins</span><br><span class="line">sudo service jenkins start</span><br></pre></td></tr></table></figure></p><p>Install plugin AWS CodePipeline<br>Install node.js:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -</span><br><span class="line">apt-get install -y nodejs</span><br><span class="line">sudo npm install -g npm</span><br><span class="line">node -v</span><br></pre></td></tr></table></figure></p><p>Install sqitch<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential cpanminus perl perl-doc</span><br><span class="line">cpanm --quiet --notest App::Sqitch</span><br><span class="line">sqitch --version</span><br><span class="line">apt-get install -y postgresql libdbd-pg-perl</span><br></pre></td></tr></table></figure></p><p>Create a new instance hapi-demo install node.js and<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install python3-pip</span><br><span class="line">sudo apt install ruby-full</span><br><span class="line">sudo apt install wget</span><br><span class="line">wget https://aws-codedeploy-ap-southeast-2.s3.amazonaws.com/latest/install</span><br><span class="line">chmod +x install</span><br><span class="line">sudo ./install auto</span><br></pre></td></tr></table></figure></p><h2 id="CodeDeploy-application"><a href="#CodeDeploy-application" class="headerlink" title="CodeDeploy application"></a>CodeDeploy application</h2><p>Create a new Codedeploy application, choose compute type ec2/on-premises, Service role-&gt; statutorialRole; Environment configuration tick Amazon EC2 instances, Key-&gt; Name, Value-&gt; hapi-demo; Deployment setting-&gt; CodeDeployDefault.OneAtATime</p><h2 id="Review-appSpec-yml-file"><a href="#Review-appSpec-yml-file" class="headerlink" title="Review appSpec.yml file"></a>Review appSpec.yml file</h2><p>appspec.yml file is an application specification file for aws codedeploy</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat appspec.yml</span><br><span class="line">version: 0.0</span><br><span class="line">os: linux</span><br><span class="line">files:</span><br><span class="line">  - source: /</span><br><span class="line">    destination: /myapp</span><br><span class="line">permissions:</span><br><span class="line">  - object: /myapp/startApp.sh</span><br><span class="line">    mode: 777</span><br><span class="line">hooks:</span><br><span class="line">  ApplicationStart:</span><br><span class="line">    - location: startApp.sh</span><br><span class="line">      timeout: 10</span><br></pre></td></tr></table></figure><h2 id="Setup-Jenkins-job"><a href="#Setup-Jenkins-job" class="headerlink" title="Setup Jenkins job"></a>Setup Jenkins job</h2><p>Create a freestyle jenkins job, configurat as following screenshots:<br><img src="https://i.imgur.com/yb9AZch.png" alt="screenshot01"><br><img src="https://i.imgur.com/jxkg9xA.png" alt="screenshot02"><br><img src="https://i.imgur.com/ghjvXwd.png" alt="screenshot03"></p><h2 id="Build-AWS-CodePieline"><a href="#Build-AWS-CodePieline" class="headerlink" title="Build AWS CodePieline"></a>Build AWS CodePieline</h2><ol><li>Source provider <a href="https://github.com/stanosaka/hapi-rest-demo">GitHub</a></li><li>Build provider: Add Jenkins; Prvider name must match the name in jenkin’s job</li><li>Deployment provider: aws codedeploy<br><img src="https://i.imgur.com/Zcps1fo.png" alt="aws codedeploy"><br><img src="https://i.imgur.com/e2nGdbj.png" alt="deployed pages"><br><img src="https://i.imgur.com/eJAhUJH.png" alt="api"><br><img src="https://i.imgur.com/MJs6nKN.png" alt="lcov-report"></li></ol><h1 id="Next-Steps"><a href="#Next-Steps" class="headerlink" title="Next Steps"></a>Next Steps</h1><h2 id="Notifications"><a href="#Notifications" class="headerlink" title="Notifications"></a>Notifications</h2><p>AWS SNS notifications for build and deployment status</p><ol><li>Create a policy named: notification-policy<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Action&quot;: &quot;sns:Publish&quot;,</span><br><span class="line">            &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>Attach notification-policy to Role CodeDeploy-EC2</p><ol start="2"><li>in CodeDeploy edit deployment group;<br><img src="https://i.imgur.com/Lx8Kxdw.png" alt="create deployment trigger"><br><img src="https://i.imgur.com/Xl4OvF8.png" alt="notification"></li></ol><h2 id="Code-changes"><a href="#Code-changes" class="headerlink" title="Code changes"></a>Code changes</h2><p>Automatically and continuously deploy code without any downtime</p><h2 id="Database-schema-changes"><a href="#Database-schema-changes" class="headerlink" title="Database schema changes"></a>Database schema changes</h2><p>Consistently and automatically deploy relational database schema changes<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqitch add product-add-comments</span><br><span class="line">cat /db/deploy/product-add-comments.sql</span><br><span class="line">-- Deploy spidertutorial:product-add-comments to pg</span><br><span class="line"></span><br><span class="line">BEGIN;</span><br><span class="line"></span><br><span class="line">-- XXX Add DDLs here.</span><br><span class="line">alter table products add comments varchar(100) default &apos;default comments&apos;;</span><br><span class="line"></span><br><span class="line">COMMIT;</span><br><span class="line">git commit -a -m &quot;add new column to products&quot;</span><br><span class="line">git push origin master:master</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Project-setup&quot;&gt;&lt;a href=&quot;#Project-setup&quot; class=&quot;headerlink&quot; title=&quot;Project setup&quot;&gt;&lt;/a&gt;Project setup&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://github.co
      
    
    </summary>
    
    
      <category term="Jenkins" scheme="http://223.95.78.227/tags/Jenkins/"/>
    
      <category term="Devops" scheme="http://223.95.78.227/tags/Devops/"/>
    
      <category term="CD/CI" scheme="http://223.95.78.227/tags/CD-CI/"/>
    
  </entry>
  
  <entry>
    <title>splunk</title>
    <link href="http://223.95.78.227/2019/07/15/splunk/"/>
    <id>http://223.95.78.227/2019/07/15/splunk/</id>
    <published>2019-07-15T00:40:04.000Z</published>
    <updated>2019-07-19T12:38:59.302Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/PacktPublishing/Splunk-7.x-Quick-Start-Guide">code</a></p><h1 id="What-is-Splunk"><a href="#What-is-Splunk" class="headerlink" title="What is Splunk?"></a>What is Splunk?</h1><p>Splunk is a software platform that collects and stores all this machine data in one place.<br><img src="https://i.imgur.com/wObsIBw.png" alt="splunk data sources and use cases"></p><h2 id="Splunk-products"><a href="#Splunk-products" class="headerlink" title="Splunk products"></a>Splunk products</h2><ul><li><strong>Splunk Enterprise</strong>: designed for on-premise deployments</li><li><strong>Splunk Cloud</strong>: a cloud-based <strong>software as a service (SaaS)</strong> version of Splunk Enterprise.</li><li><strong>Splunk Light</strong>: is designed to be a small-scale solution.</li><li><strong>Splunk Free</strong>: is a free version of the core Splunk Enterprise product that has limits on users(one user), ingestion volume (500 MB/day), and other features.</li></ul><p><strong>Splunk components</strong></p><ul><li>Universal forwarder</li><li>Indexer and indexer clusters</li><li>Search head and search head clusters</li><li>Deployment server</li><li>Deployer</li><li>Cluster master</li><li>License master</li><li>Heavy forwarder<br>Universal forwarders, indexers, and search heads constitute the majority of Splunk functionality; the other components provide supporting roles for larger clustered/distributed environments.<br><img src="https://i.imgur.com/sqJ8esH.png" alt="Splunk components in a distributed deployment"></li></ul><p>The <strong>universal forwarder (UF)</strong> is a free small-footprint version of Splunk Enterprise that is installed on each application, web, or other type of server to collect data from specified log files and forward this data to Splunk for indexing(storage). In A large Splunk deployment, you may have hundreds or thousands of forwards that consume and forward data for indexing.</p><p>An <strong>indexer</strong> is the Splunk component that creates and manages indexes, which is where machine data is stored. Indexers perform two main functions: parsing and storing data, which has been received from forwarders or other data sources into indexes, and searching and returning the indexed data in response to search requests.</p><p>An indexing cluster is a group of indexers that have been configured to work together to handle higher volumes of both incmoing data to be indexed and search requests to be serviced, as well as providing redundancy by keeping duplicate copies of indexed data spread across the cluster members.</p><p>A <strong>search head</strong> is an instance of Splunk Enterprise that handles search management functions. This includes providing a web-based user interface called Splunk Web, from which users issue search requests in what is called <strong>Search Processing Language (SPL)</strong>. Search reqeusts initiated by a user ( or a report or dashboard) are sent to one or more indexers to locate and return the requested data; the search head then formates the returned data for presentation to the user.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">index=_internal | stats count by source, sourcetype</span><br></pre></td></tr></table></figure><p>an example of executing a simple search in Splunk Web. The SPL specifies searching in the <code>_internal</code> index, which is where Splunk saves data about its internal operations, and provides a count of the number of events in each log for Today. The SPL command specified an <code>index</code>, and then pipes the returned results to the <code>stats</code> command to return a <code>count</code> of all the events by their <code>source</code> and `sourcetype<br><img src="https://i.imgur.com/exMXaN5.jpg" alt="Simple search in Splunk Web"></p><p>A <strong>deployment server</strong> is a Splunk Enterprise instance that acts as a centralized configuration manager ofr a number of Splunk components, but which in practice is used to manage UFs.</p><p>A <strong>deployer</strong> is a Splunk Enterprise instance that is ued to distribute Splunk apps and certain other configuration updates to search head cluster memebers.</p><p>A <strong>cluster master</strong> is a Splunk Enterprise instance that coordinates the activities of an indexing cluster.</p><p>A <strong>license master</strong> is a single Splunk Enterprise instance that provides a licensing service for the multiple instances of Splunk that have been deployed in a distributed environment.</p><p>A <strong>heavy forwarder</strong> is an instance of Splunk Enterprise that can receive data from other forwarders or data sources and parse, index, and/or send data to another Splunk instance for indexing. </p><p>Splunk Enterprise also has a monitoring tool function called the <strong>monitoring console</strong>, which lets you view detailed topology and performance information about your entire distributed deployment from one interface. </p><p><img src="https://i.imgur.com/ElU7rTv.png" alt="Splunk data pipeline"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/PacktPublishing/Splunk-7.x-Quick-Start-Guide&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;What-is-Splunk&quot;&gt;&lt;a href=&quot;#What-is-Splunk&quot; 
      
    
    </summary>
    
    
      <category term="splunk" scheme="http://223.95.78.227/tags/splunk/"/>
    
      <category term="monitoring" scheme="http://223.95.78.227/tags/monitoring/"/>
    
  </entry>
  
</feed>
